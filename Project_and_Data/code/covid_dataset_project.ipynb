{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FoDS Project: Covid "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure\n",
    "1. Environment\n",
    "2. Data import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Environment\n",
    "1. python == 3.11.8 \n",
    "2. matplotlib=3.8.3\n",
    "3. numpy=1.26.0\n",
    "4. pandas=2.1.1\n",
    "5. scipy=1.12.0\n",
    "6. seaborn=0.13.2\n",
    "7. sklearn=1.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as sts\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.impute import KNNImputer\n",
    "from scipy.interpolate import interp1d\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/OWID-covid-data-28Feb2023.csv\")\n",
    "df = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Getting an overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure\n",
    "1. Basic overview\n",
    "2. Datatypes\n",
    "3. Locations and continents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 260567 entries, 0 to 260566\n",
      "Data columns (total 67 columns):\n",
      " #   Column                                      Non-Null Count   Dtype  \n",
      "---  ------                                      --------------   -----  \n",
      " 0   iso_code                                    260567 non-null  object \n",
      " 1   continent                                   245944 non-null  object \n",
      " 2   location                                    260567 non-null  object \n",
      " 3   date                                        260567 non-null  object \n",
      " 4   total_cases                                 246027 non-null  float64\n",
      " 5   new_cases                                   245704 non-null  float64\n",
      " 6   new_cases_smoothed                          244500 non-null  float64\n",
      " 7   total_deaths                                226322 non-null  float64\n",
      " 8   new_deaths                                  226220 non-null  float64\n",
      " 9   new_deaths_smoothed                         225034 non-null  float64\n",
      " 10  total_cases_per_million                     244910 non-null  float64\n",
      " 11  new_cases_per_million                       244587 non-null  float64\n",
      " 12  new_cases_smoothed_per_million              243388 non-null  float64\n",
      " 13  total_deaths_per_million                    225218 non-null  float64\n",
      " 14  new_deaths_per_million                      225116 non-null  float64\n",
      " 15  new_deaths_smoothed_per_million             223935 non-null  float64\n",
      " 16  reproduction_rate                           184817 non-null  float64\n",
      " 17  icu_patients                                34705 non-null   float64\n",
      " 18  icu_patients_per_million                    34705 non-null   float64\n",
      " 19  hosp_patients                               37783 non-null   float64\n",
      " 20  hosp_patients_per_million                   37783 non-null   float64\n",
      " 21  weekly_icu_admissions                       8715 non-null    float64\n",
      " 22  weekly_icu_admissions_per_million           8715 non-null    float64\n",
      " 23  weekly_hosp_admissions                      20606 non-null   float64\n",
      " 24  weekly_hosp_admissions_per_million          20606 non-null   float64\n",
      " 25  total_tests                                 79387 non-null   float64\n",
      " 26  new_tests                                   75403 non-null   float64\n",
      " 27  total_tests_per_thousand                    79387 non-null   float64\n",
      " 28  new_tests_per_thousand                      75403 non-null   float64\n",
      " 29  new_tests_smoothed                          103965 non-null  float64\n",
      " 30  new_tests_smoothed_per_thousand             103965 non-null  float64\n",
      " 31  positive_rate                               95927 non-null   float64\n",
      " 32  tests_per_case                              94348 non-null   float64\n",
      " 33  tests_units                                 106788 non-null  object \n",
      " 34  total_vaccinations                          72780 non-null   float64\n",
      " 35  people_vaccinated                           69684 non-null   float64\n",
      " 36  people_fully_vaccinated                     67025 non-null   float64\n",
      " 37  total_boosters                              41511 non-null   float64\n",
      " 38  new_vaccinations                            60372 non-null   float64\n",
      " 39  new_vaccinations_smoothed                   156739 non-null  float64\n",
      " 40  total_vaccinations_per_hundred              72780 non-null   float64\n",
      " 41  people_vaccinated_per_hundred               69684 non-null   float64\n",
      " 42  people_fully_vaccinated_per_hundred         67025 non-null   float64\n",
      " 43  total_boosters_per_hundred                  41511 non-null   float64\n",
      " 44  new_vaccinations_smoothed_per_million       156739 non-null  float64\n",
      " 45  new_people_vaccinated_smoothed              156661 non-null  float64\n",
      " 46  new_people_vaccinated_smoothed_per_hundred  156661 non-null  float64\n",
      " 47  stringency_index                            185194 non-null  float64\n",
      " 48  population_density                          227138 non-null  float64\n",
      " 49  median_age                                  209984 non-null  float64\n",
      " 50  aged_65_older                               207771 non-null  float64\n",
      " 51  aged_70_older                               208886 non-null  float64\n",
      " 52  gdp_per_capita                              209771 non-null  float64\n",
      " 53  extreme_poverty                             136558 non-null  float64\n",
      " 54  cardiovasc_death_rate                       209355 non-null  float64\n",
      " 55  diabetes_prevalence                         220404 non-null  float64\n",
      " 56  female_smokers                              158843 non-null  float64\n",
      " 57  male_smokers                                156670 non-null  float64\n",
      " 58  handwashing_facilities                      103305 non-null  float64\n",
      " 59  hospital_beds_per_thousand                  186728 non-null  float64\n",
      " 60  life_expectancy                             239175 non-null  float64\n",
      " 61  human_development_index                     204714 non-null  float64\n",
      " 62  population                                  259450 non-null  float64\n",
      " 63  excess_mortality_cumulative_absolute        8649 non-null    float64\n",
      " 64  excess_mortality_cumulative                 8649 non-null    float64\n",
      " 65  excess_mortality                            8649 non-null    float64\n",
      " 66  excess_mortality_cumulative_per_million     8649 non-null    float64\n",
      "dtypes: float64(62), object(5)\n",
      "memory usage: 133.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of observations:\", data.shape[0])\n",
    "print(\"Number of columns:\", data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Datatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 All datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datatypes of each variable:\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_data = data.select_dtypes(include=(\"int64\")).columns\n",
    "print(\"INTEGERS: \", int_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we do not have any integers in our data. (We have but they are just not saved as such)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_data = data.select_dtypes(include=(\"float64\")).columns\n",
    "print(\"FLOATS: \", float_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_data = data.select_dtypes(include=(\"object\")).columns\n",
    "print(\"OBJECTS:\", object_data)\n",
    "num_data = data[float_data] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Saving numerical and categorical data in seperate data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = data[float_data] \n",
    "cat_data = data[object_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Adjusting date datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"date\"] = pd.to_datetime(data[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Introductory Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure\n",
    "1. Cases Visualized\n",
    "2. Deaths Visualized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cases Visualized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Cases per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: Most countries did not publish covid numbers on the weekends\n",
    "# Effect: This leads to an oscillating graph for daily new cases\n",
    "# This can be solved with weekly new cases\n",
    "# either extract data from new_daily or use the smoothed cases count\n",
    "daily_cases = data.groupby('date')['new_cases'].sum()\n",
    "data_march= data[(data['date']>'2022-03-01')&(data['date']<'2022-04-01')]\n",
    "daily_cases_march = data_march.groupby('date')['new_cases'].sum()\n",
    "daily_cases_weekly= data.groupby(pd.Grouper(key='date', freq='W')).sum()['new_cases']\n",
    "\n",
    "# Plotting\n",
    "fig, (ax1,ax2,ax3, ax4)=plt.subplots(4,1,figsize=(10,20))\n",
    "ax1.plot(daily_cases, label='New Cases Per Day')\n",
    "ax1.set(title='COVID-19 New Cases Per Day',\n",
    "        xlabel='Date',\n",
    "        ylabel='Number of New Cases'\n",
    "        )\n",
    "ax1.legend()\n",
    "ax2.plot(daily_cases_march, label='New Cases Per Day, March 2022')\n",
    "ax2.set(title='COVID-19 New Cases Per Day',\n",
    "        xlabel='Date',\n",
    "        ylabel='Number of New Cases'\n",
    "        )\n",
    "ax2.legend()\n",
    "ax3.plot(daily_cases_weekly, label='New Cases Per Day, Daily turned to weekly')\n",
    "ax3.set(title='COVID-19 New Cases Per Day',\n",
    "        xlabel='Date',\n",
    "        ylabel='Number of New Cases'\n",
    "        )\n",
    "\n",
    "ax3.legend()\n",
    "ax4.plot(data.groupby('date')['new_cases_smoothed'].sum(), label='New Cases Per Day, Daily turned to weekly')\n",
    "ax4.set(title='COVID-19 New Cases Per Day',\n",
    "        xlabel='Date',\n",
    "        ylabel='Number of New Cases'\n",
    "        )\n",
    "\n",
    "ax4.legend()\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.savefig('../output/Introductory_Visualizations/Cases/newcasestotal.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Cases over time by continent compact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data[~(data['continent']==0)].groupby(['date', 'continent'])['new_cases_smoothed'].sum().reset_index()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.lineplot(x='date', y='new_cases_smoothed', hue='continent', data=grouped_data)\n",
    "plt.title('New COVID-19 Cases by Continent Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of New Cases')\n",
    "plt.legend(title='Continent')\n",
    "plt.grid(True)  \n",
    "plt.savefig('../output/Introductory_Visualizations/Cases/newcasesbycontinent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Cases over time by continent in subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data[~(data['continent']==0)].groupby(['continent','date'])['new_cases_smoothed'].sum().reset_index()\n",
    "continents_of_interest=['North America','South America','Asia','Europe','Oceania','Africa']\n",
    "fig, axs = plt.subplots(1,6,figsize=(24,8), sharey=True)\n",
    "for i,continent in enumerate(continents_of_interest):\n",
    "        sns.lineplot(x='date', y='new_cases_smoothed',  data=grouped_data[grouped_data['continent']==continent],ax=axs[i])\n",
    "        axs[i].set_xlabel('Date')\n",
    "        axs[i].set_ylabel('New Cases')\n",
    "        axs[i].set_title(continent)\n",
    "        axs[i].tick_params(axis='x', rotation=90)\n",
    "        axs[i].grid(True)\n",
    "plt.suptitle('New Cases by Continent')\n",
    "plt.savefig('../output/Introductory_Visualizations/Cases/casesbycontinentsp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Total cases vs total cases per million by continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases=['total_cases','total_cases_per_million']\n",
    "fig, axs= plt.subplots(2,1,figsize=(10,10))\n",
    "data_nozero=data[~(data['continent']==0)]\n",
    "for i, variable in enumerate(cases):\n",
    "        top_countries = data_nozero.groupby('continent')[variable].max().nlargest(10)\n",
    "        print(top_countries)\n",
    "        top_countries.plot(kind='bar', ax=axs[i],grid=True)\n",
    "        axs[i].set_xlabel('Continent')\n",
    "        axs[i].set_ylabel(variable)\n",
    "plt.suptitle('Total Cases vs Total Cases per Million')\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.savefig('../output/Introductory_Visualizations/Cases/casesbycontinent.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Total cases vs total cases per million by country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases=['total_cases','total_cases_per_million']\n",
    "fig, axs= plt.subplots(2,1,figsize=(10,10))\n",
    "data_noowid=data[~data['iso_code'].str.startswith(\"OWID_\")]\n",
    "for i, variable in enumerate(cases):\n",
    "        top_countries = data_noowid.groupby('iso_code')[variable].max().nlargest(10)\n",
    "        print(top_countries)\n",
    "        top_countries.plot(kind='bar', ax=axs[i],grid=True)\n",
    "        axs[i].set_xlabel('Country')\n",
    "        axs[i].set_ylabel(variable)\n",
    "plt.suptitle('Total Cases vs Total Cases per Million')\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.savefig('../output/Introductory_Visualizations/Cases/casesbycountry.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Scatter plots with variables of interest to see correlation with total cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_of_interest = [\n",
    "    'icu_patients_per_million', 'total_tests_per_thousand', 'total_vaccinations_per_hundred',\n",
    "    'stringency_index', 'hospital_beds_per_thousand', 'aged_65_older'\n",
    "]\n",
    "fig, axs = plt.subplots(3, 2, figsize=(15, 15))\n",
    "axs = axs.flatten()\n",
    "for i, variable in enumerate(variables_of_interest):\n",
    "    sns.scatterplot(x=variable, y='total_cases_per_million', data=data, ax=axs[i], markers='.')\n",
    "    axs[i].set_title(f'Total Cases per Million vs. {variable}')\n",
    "    axs[i].set_xlabel(variable)\n",
    "    axs[i].set_ylabel('Total Cases per Million')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/Introductory_Visualizations/Cases/totcasescorr.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Deaths visualized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 New deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_deaths = data.groupby('date')['new_deaths'].sum()\n",
    "daily_deaths_smoothed = data.groupby('date')['new_deaths_smoothed'].sum()\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(2,1,figsize=(10,20))\n",
    "\n",
    "ax1.plot(daily_deaths, label='New Deaths Per Day')\n",
    "ax1.set(title='COVID-19 New Deaths Per Day', xlabel='Date', ylabel='Number of New Deaths')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(daily_deaths_smoothed, label='New Deaths Per Day, Smoothed Data')\n",
    "ax2.set(title='COVID-19 New Deaths Per Day', xlabel='Date', ylabel='Number of New Deaths')\n",
    "ax2.legend()\n",
    "\n",
    "plt.savefig(\"../output/Introductory_Visualizations/Deaths/new_deaths.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to find out if there is a difference in deaths per million among the continents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_deaths_per_million_continent = data.groupby(['date', 'continent'])['new_deaths_per_million'].sum().reset_index()\n",
    "new_deaths_per_million_continent_smoothed = data.groupby(['date', 'continent'])['new_deaths_smoothed_per_million'].sum().reset_index()\n",
    "total_cases_per_continent = data.groupby(['date', 'continent'])['total_cases_per_million'].sum().reset_index()\n",
    "print(new_deaths_per_million_continent)\n",
    "print(new_deaths_per_million_continent_smoothed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 New deaths per million by continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continents_deaths = new_deaths_per_million_continent['continent'].unique()\n",
    "#Creating suplots per Continent with the real counts\n",
    "fig, axes = plt.subplots(len(continents_deaths), 1, figsize=(10, 6*len(continents_deaths)), sharex=True)\n",
    "# Iterate over each continent and create a subplot\n",
    "for i, continent in enumerate(continents_deaths):\n",
    "    continent_data = new_deaths_per_million_continent[new_deaths_per_million_continent['continent'] == continent]\n",
    "    sns.lineplot(data=continent_data, x='date', y='new_deaths_per_million', ax=axes[i])\n",
    "    axes[i].set_title(f'New Deaths per Million in {continent} Over Time')\n",
    "    axes[i].set_ylabel('New Deaths per Million')\n",
    "    axes[i].set_xlabel('Date')\n",
    "    axes[i].grid(True)\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"../output/Introductory_Visualizations/Deaths/NewDeathsPerMillion_Subplots.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Smoothed new deaths per million by continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(continents_deaths), 1, figsize=(10, 6*len(continents_deaths)), sharex=True)\n",
    "\n",
    "for i, continent in enumerate(continents_deaths):\n",
    "    continent_data = new_deaths_per_million_continent_smoothed[new_deaths_per_million_continent_smoothed['continent'] == continent]\n",
    "    sns.lineplot(data=continent_data, x='date', y='new_deaths_smoothed_per_million', ax=axes[i])\n",
    "    axes[i].set_title(f'New Deaths per Million in {continent} Over Time')\n",
    "    axes[i].set_ylabel('New Deaths per Million')\n",
    "    axes[i].set_xlabel('Date')\n",
    "    axes[i].grid(True)\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"../output/Introductory_Visualizations/Deaths/NewDeathsPerMillionSmoothed_Subplots.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 New deaths per million divided by total cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to make a statement about the severity by analyzing the new deaths per million divided by the total cases.\n",
    "Merge the new deaths and total cases. Calculate new deaths per million divided by total cases per million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_cases_merge = pd.merge(new_deaths_per_million_continent_smoothed, total_cases_per_continent, on=['date', 'continent'])\n",
    "death_cases_merge['deaths_to_cases_ratio'] = death_cases_merge['new_deaths_smoothed_per_million'] / death_cases_merge['total_cases_per_million']\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(len(continents_deaths), 1, figsize=(10, 6*len(continents_deaths)), sharex=True)\n",
    "\n",
    "for i, continent in enumerate(continents_deaths):\n",
    "    continent_data = death_cases_merge[death_cases_merge['continent'] == continent]\n",
    "    sns.lineplot(data=continent_data, x='date', y='deaths_to_cases_ratio', ax=axes[i])\n",
    "    axes[i].set_title(f'New Deaths per Million divided by Total Cases per Million in {continent} Over Time')\n",
    "    axes[i].set_ylabel('New Deaths per Million / Total Cases per Million')\n",
    "    axes[i].set_xlabel('Date')\n",
    "    axes[i].grid(True)\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"../output/Introductory_Visualizations/Deaths/NewDeathsToCasesRatio_Subplots.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Missing Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introductory remarks about OWID\n",
    "2. Preparatory steps\n",
    "3. First overview of missing data\n",
    "4. Overview missing data \"cases\" and \"deaths\"\n",
    "5. Total cases continent analysis\n",
    "6. Factual accuracy\n",
    "7. Country analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introductory remarks about OWID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OWID itself states that it generally takes its data from 4 different sources:\n",
    "1. Specialized institutes\n",
    "2. Research articles\n",
    "3. International Institutations or statistics agencies\n",
    "4. Official data from government sources\n",
    "\n",
    "More specifically, looking at the OWID COVID dataset online where all the sources are indicated, it can be seen that OWID takes most of the data directly from the WHO COVID dashboard and then completes it with other sources which is why this dataset can be expected to be quite complete. However, the following analysis is attempting to analyze how much data is missing from the dataset and what kind of data is missing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preparatory steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Adjusting visualization settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colorblind-friendly palette\n",
    "color_palette = sns.color_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjusting the viewing options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicates\n",
    "data = data.drop_duplicates()\n",
    "#separating OWID from non_OWID data\n",
    "data_without_OWID = data[~data['iso_code'].str.contains('OWID')]\n",
    "data_OWID = data[data['iso_code'].str.contains('OWID')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What OWID contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_OWID_loc = data_OWID.groupby(\"location\")\n",
    "loc_OWID = list(grouped_OWID_loc.groups.keys())\n",
    "print(loc_OWID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OWID contains a few extra metrics and countries/continents or composites of such."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. First overview of missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Entries with complete information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For how many entries do we have complete information?\n",
    "(data.isna().sum(axis=1) == 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Missing values per column overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing values overall per column\n",
    "total_missing = data.isnull().sum()\n",
    "total_missing_percentage = (total_missing / len(data)) * 100\n",
    "total_missing_sorted = total_missing.sort_values()\n",
    "total_missing_percentage_sorted = total_missing_percentage.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graphing the missing values per column\n",
    "plt.figure(figsize=(6, 8))\n",
    "fig_mv_variables = sns.barplot(\n",
    "    x = total_missing_percentage_sorted.values,\n",
    "    y = total_missing_percentage_sorted.index\n",
    ")\n",
    "fig_mv_variables.set(\n",
    "    title=\"Missing values per variable\",\n",
    "    xlabel=\"Percentage of Missing Values\",\n",
    "    ylabel=\"Variables\"\n",
    ")\n",
    "plt.savefig(\"../output/Missing_Data_Analysis/missing_data.png\", bbox_inches=\"tight\")              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Overview missing data \"cases\" and \"deaths\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Selecting only the columns containing information about cases and deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting the columns about cases of interest\n",
    "columns_about_cases = data.loc[:, data.columns.str.contains('cases')]\n",
    "columns_about_cases.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_about_deaths = data.loc[:, data.columns.str.contains('death')]\n",
    "columns_about_deaths.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Sorting and plotting overall missing data about cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_data_filtered(columns_of_interest):\n",
    "    total_missing = columns_of_interest.isnull().sum()\n",
    "    total_missing_percentage = (total_missing / len(data)) * 100\n",
    "    total_missing_sorted = total_missing.sort_values()\n",
    "    total_missing_percentage_sorted = total_missing_percentage.sort_values()\n",
    "    return total_missing_percentage_sorted\n",
    "\n",
    "total_missing_percentage_deaths_sorted = missing_data_filtered(columns_about_deaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "deaths_overview = sns.barplot(\n",
    "    x = total_missing_percentage_deaths_sorted.values,\n",
    "    y = total_missing_percentage_deaths_sorted.index\n",
    ")\n",
    "deaths_overview.set(\n",
    "    title=\"Missing values per variable\",\n",
    "    xlabel=\"Percentage of Missing Values\",\n",
    "    ylabel=\"Variables\"\n",
    ")\n",
    "plt.savefig(\"../output/Missing_Data_Analysis/deathsmissing_data.png\", bbox_inches=\"tight\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Sorting and plotting overall missing data about deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_missing_percentage_cases_sorted = missing_data_filtered(columns_about_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "cases_overview = sns.barplot(\n",
    "    x = total_missing_percentage_cases_sorted.values,\n",
    "    y = total_missing_percentage_cases_sorted.index\n",
    ")\n",
    "cases_overview.set(\n",
    "    title=\"Missing values per variable\",\n",
    "    xlabel=\"Percentage of Missing Values\",\n",
    "    ylabel=\"Variables\"\n",
    ")\n",
    "plt.savefig(\"../output/Missing_Data_Analysis/casesmissing_data.png\", bbox_inches=\"tight\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is in the overall dataset a lot more data around cases than deaths. However, amongst the columns that give information about either death or cases, the variability as to the amount of missing data is low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Total cases continent analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Amount of data per country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: Since reporting is generally done on a national level, it makes sense to have a look at individual countries to find out where potential data is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many countries are we dealing with overall\n",
    "unique_values_world = data_without_OWID['iso_code'].unique()\n",
    "print (len(unique_values_world))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the number of entries for each country\n",
    "length_list = []\n",
    "length_list_in_years = []\n",
    "\n",
    "for i in range(len(unique_values_world)):\n",
    "    country = unique_values_world[i]\n",
    "    length_country = len(data_without_OWID.loc[df['iso_code'] == country])\n",
    "    length_list.append(length_country)\n",
    "    length_list_in_years.append(length_country/365)\n",
    "\n",
    "sorted_country_length = sorted(length_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(length_list_in_years, bins=30, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('data points converted to number of years')\n",
    "plt.ylabel('number of countries')\n",
    "plt.title(\"Number of entries in the data set per country\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Total cases Asia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different continents\n",
    "filtered_continents = data_without_OWID[\"continent\"].unique()\n",
    "filtered_continents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function returning list of all different countries on one continent\n",
    "def countries_per_continent(continent):\n",
    "    filtered_continent = data_without_OWID.loc[df['continent'] == continent]\n",
    "    unique_values_continent = filtered_continent['iso_code'].unique()\n",
    "    return unique_values_continent\n",
    "\n",
    "unique_values_Asia = countries_per_continent(\"Asia\")\n",
    "unique_values_Asia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continent_values(continent_uniq_val):\n",
    "    values = []\n",
    "    for i in range(len(continent_uniq_val)):\n",
    "        currentcountry = continent_uniq_val[i]\n",
    "        filtered_country = data.loc[df['iso_code'] == currentcountry]\n",
    "        no_rows_current_country = len(filtered_country)\n",
    "        missing = (filtered_country[\"total_cases\"].isnull().sum())\n",
    "        total_missing_percentage_current_country = (missing / len(filtered_country)) * 100\n",
    "        values.append(total_missing_percentage_current_country)\n",
    "    return values\n",
    "\n",
    "values_Asia = continent_values(unique_values_Asia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_barplot_continent (unique_codes, values):\n",
    "    plt.figure(figsize=(26, 8))\n",
    "    missing_values_Asia = sns.barplot(\n",
    "        x = unique_codes,\n",
    "        y = values\n",
    "    )\n",
    "\n",
    "    deaths_overview.set(\n",
    "        title=\"Missing values per variable\",\n",
    "        xlabel=\"Percentage of Missing Values\",\n",
    "        ylabel=\"Variables\"\n",
    "    )\n",
    "\n",
    "    plt.savefig(\"../output/Missing_Data_Analysis/casesmissing_data2.png\", bbox_inches=\"tight\") \n",
    "\n",
    "plotting_barplot_continent(unique_values_Asia, values_Asia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Total Cases Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values_Europe = countries_per_continent(\"Europe\")\n",
    "unique_values_Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_Europe = continent_values(unique_values_Europe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_barplot_continent(unique_values_Europe, values_Europe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Total Cases Africa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values_Africa = countries_per_continent(\"Africa\")\n",
    "unique_values_Africa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_Africa = continent_values(unique_values_Africa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_barplot_continent(unique_values_Africa, values_Africa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5 Total Cases North America"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values_North_America = countries_per_continent(\"North America\")\n",
    "unique_values_North_America"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_North_America = continent_values(unique_values_North_America)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_barplot_continent(unique_values_North_America, values_North_America)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Factual accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Do total cases and new cases lead to the same numbers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at Canada the reporting started on 23 of January but the first death was on March 9. A quick search on the internet confirms that 9 March was the date on which the frist Canadian person died. That makes 46 days on which there is no data on deaths. But there are 1132 entries for Canada and hence 46/1132 = 4%. In the case of Switzerland 10 no values for deaths and 1099 rows so that would make 1% of the data missing. For Germany 42/1128 = 4%. All of this data is not missing, it is just wrongly classified as NA when these should be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_total_new_cases(country): \n",
    "    filtered_country = data_without_OWID.loc[df[\"location\"] == \"Sweden\"]\n",
    "    filtered_country_total_cases_from_tc = filtered_country.iloc[-1, 4]\n",
    "    print (\"total cases from the total cases column: \", filtered_country_total_cases_from_tc)\n",
    "    print (\"total cases when summing over new cases: \", filtered_country[\"new_cases\"].sum())\n",
    "\n",
    "comparison_total_new_cases(\"Sweden\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 How accurate are vaccination statistics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Wikipedia, the first vaccination in Switzerland, Germany and France tool place on December 23 2020, December 22, 2020 and December 17, 2020 respectively. The dataset confirms these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_vaccination_statistics_accuracy(country):\n",
    "    filtered_country = data_without_OWID.loc[df['location'] == country]\n",
    "    filtered_country = filtered_country.reset_index(drop=True)\n",
    "    first_vaccination_index = filtered_country[\"total_vaccinations\"].first_valid_index()\n",
    "    return filtered_country.iloc[first_vaccination_index,:5]\n",
    "    \n",
    "first_vaccination_statistics_accuracy(\"France\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Country analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Function to replace missing data with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here is how to replace the mssing values with zero values\n",
    "def replace_missing_data(country):\n",
    "    filtered_country = data_without_OWID.loc[df['location'] == country]\n",
    "    #filtered_country = filtered_Italy.reset_index(drop=True)\n",
    "    missing_vals = [\"NA\", \"\", None, np.NaN]\n",
    "    missing_country = filtered_country.isin(missing_vals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Function for missing values excluding missing at the end and at the beginning for a specific country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering_missing_country(country):\n",
    "    filtered_country = data_without_OWID.loc[df['location'] == country]\n",
    "    filtered_country = filtered_country.reset_index(drop=True)\n",
    "    filtered_country_summary = filtered_country.isnull().sum()\n",
    "    first_valid_indices_country = filtered_country.apply(lambda x: x.first_valid_index())\n",
    "    last_valid_indices_country = filtered_country.apply(lambda x: x.last_valid_index())\n",
    "    length = len(filtered_country) - 1\n",
    "\n",
    "    filtered_country_summary_df = filtered_country_summary.to_frame()\n",
    "    first_valid_indices_country_df = first_valid_indices_country.to_frame()\n",
    "    last_valid_indices_country_df = last_valid_indices_country.to_frame()\n",
    "    list_of_dataframes = [filtered_country_summary_df, first_valid_indices_country_df, last_valid_indices_country]\n",
    "\n",
    "    comparison_country = pd.merge(filtered_country_summary_df, first_valid_indices_country_df, left_index=True, right_index=True)\n",
    "    comparison_country = comparison_country.rename(columns={\"0_x\": '# of missing entries', \"0_y\": \"first entry\"})\n",
    "    diff_first = filtered_country_summary_df - first_valid_indices_country_df\n",
    "    last_valid_indices_country_df_diff = length - last_valid_indices_country_df\n",
    "    diff_overall = filtered_country_summary_df - first_valid_indices_country_df - last_valid_indices_country_df_diff\n",
    "    comparison_country = comparison_country.assign(diff_first = diff_first, \\\n",
    "                last_entry= last_valid_indices_country_df, diff_last = last_valid_indices_country_df_diff, \\\n",
    "                diff_overall = diff_overall)\n",
    "    return (comparison_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering_missing_country(\"Finland\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 Function total diff for many countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locations_per_continent(continent):\n",
    "    filtered_continent = data_without_OWID.loc[df['continent'] == continent]\n",
    "    unique_values_continent = filtered_continent['location'].unique()\n",
    "    return unique_values_continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_locations_Europe = locations_per_continent(\"Europe\")\n",
    "empty_df = pd.DataFrame()\n",
    "\n",
    "def several_countries(list, dataframe):\n",
    "    count = 0\n",
    "    for i in (list):\n",
    "        country = filtering_missing_country(i)\n",
    "        #print (country)\n",
    "        country_diff_overall_column = country[\"diff_overall\"]\n",
    "        #print (country_diff_overall_column)\n",
    "        country_diff_overall_column_df = country_diff_overall_column.to_frame()\n",
    "        if count == 0:\n",
    "            combined_df = pd.concat([empty_df, country_diff_overall_column_df], axis=1)\n",
    "            count = 1\n",
    "        elif count > 0:\n",
    "            combined_df = pd.concat([combined_df, country_diff_overall_column_df], axis=1)\n",
    "        #print (country_diff_overall_column_df)\n",
    "        combined_df = combined_df.rename(columns={'diff_overall': i})\n",
    "    return (combined_df)\n",
    "\n",
    "several_countries(unique_locations_Europe, empty_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_of_interest = \"Finland\"\n",
    "filtered = data_without_OWID.loc[df['location'] == country_of_interest]\n",
    "filtered = filtered.reset_index(drop=True)\n",
    "vaccinations = filtered[\"total_vaccinations\"]\n",
    "vaccinations.head(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finland and other countries only reporting once per week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Handling of missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure\n",
    "1. Decision to delete columns 63:67\n",
    "2. Decision to delete entire countries\n",
    "3. Decision over keeping or deleting/imputing columns 0:16 (Nico)\n",
    "4. Decision over keeping or deleting/imputing columns 16:32 (Leon)\n",
    "5. Decision over keeping or deleting/imputing columns 32:48 (Florin)\n",
    "6. Decision over keeping or deleting/imputing columns 48:63 (Sevi)\n",
    "7. Actual imputations and deletions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Decision to delete columns 63:67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_percentages = df.iloc[:,63:].isnull().sum() / df.shape[0] *100\n",
    "print(missing_percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than 95% of entries are missing. These four columns will be deleted entirely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decision to delete entire countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 What countries miss how many rows entirely?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What countries are missing entire columns? Helps us determine if we should maybe just remove the countries instead of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data = []\n",
    "for column in df.columns:\n",
    "    grouped_by_iso = df.groupby(\"iso_code\")\n",
    "    nan_counts = grouped_by_iso[column].apply(lambda x: x.isnull().all())\n",
    "    result_data.extend([{'ISO_Code': iso_code, 'Column': column, 'All_NaN': all_nan} for iso_code, all_nan in zip(nan_counts.index, nan_counts.values)])\n",
    "\n",
    "result_df = pd.DataFrame(result_data)\n",
    "most_true_iso_code = result_df.loc[result_df['All_NaN']].groupby('ISO_Code').size().sort_values(ascending=False)\n",
    "\n",
    "print(\"ISO code with the most occurrences of completely missing entire columns in their respective row:\", most_true_iso_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OWID seems more and more useless. Maybe also delete other countries entirely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to find out where to make the cutoff point for the countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20_missing_complete_loc= most_true_iso_code.head(20)\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.bar(top20_missing_complete_loc.index, top20_missing_complete_loc)\n",
    "plt.xlabel(\"iso codes\")\n",
    "plt.ylabel(\"Number of completely missing columns\")\n",
    "plt.tight_layout()\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This topic of row/country deletion has to be visited again after deleting columns that can be deleted for sure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Decision over keeping or deleting/imputing columns 0:16 (Nico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Missing data percentages (Nico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_nico = data.columns[0:16]\n",
    "missing_data_nico = df[columns_nico].isnull().sum() \n",
    "missing_data_nico_percentage = missing_data_nico / df.shape[0] * 100\n",
    "print(missing_data_nico_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the first 4 columns are all (except continent) complete. This makes sense since they contain information about the country and date and did not have to be measured.\n",
    "In general the amount of missing values is very low. For this reason and since cases and deaths are our Label, we cannot delete a column from this part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Missing data per country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we will look if there are certain countries missing a huge amount of data. If that were the case we could delete the entries of those couuntries. To get an overview I created a plot for each continent containing all the countries and their respective missing entries in the first 16 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pd.DataFrame(filtering_missing_country('Finland'))['# of missing entries'][0:16]) \n",
    "#print(locations_per_continent('Europe'))\n",
    "\n",
    "def missingdata_country_nico(continent):\n",
    "    missing_data_countrylist_nico = []\n",
    "    \n",
    "    for country in locations_per_continent(continent):\n",
    "        missing_data_country_nico = pd.DataFrame(filtering_missing_country(country))\n",
    "        sum_missing_entries_nico = missing_data_country_nico['# of missing entries'][0:16].sum()\n",
    "        missing_data_countrylist_nico.append({'Country': country, 'Missing Entries': sum_missing_entries_nico})\n",
    "        #print(f\"{country}: {sum_missing_entries_nico}\")\n",
    "    missing_data_country_df_nico = pd.DataFrame(missing_data_countrylist_nico)\n",
    "    \n",
    "    # Create countplot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Country', y='Missing Entries', data=missing_data_country_df_nico)\n",
    "    plt.title('Missing Entries per Country in ' + continent)\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x labels for better readability\n",
    "    plt.xlabel('Country')\n",
    "    plt.ylabel('Number of Missing Entries')\n",
    "    plt.tight_layout()\n",
    "    safepoint = \"../output/Missing_Data_Analysis/cols0-16/missing_data-\" + continent +\".png\"\n",
    "    plt.savefig(safepoint)\n",
    "\n",
    "\n",
    "for continent in continents_of_interest:\n",
    "    missingdata_country_nico(continent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clearly visible that there are a few outliers, which have a lot of missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part of the code I wanted to check if the 'per million' numbers are deducted from the total numbers. This was done with the total cases and toatal cases per million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in locations_per_continent('Europe'):\n",
    "    country_entries_totalcases = df[df[columns_nico[2]] == country].iloc[:, 4]\n",
    "    country_entries_totalcasesmillion = df[df[columns_nico[2]] == country].iloc[:, 10]\n",
    "    country_entries_missing_totalcases = country_entries_totalcases.isnull().sum()\n",
    "    country_entries_missing_totalcasesmillion = country_entries_totalcasesmillion.isnull().sum()\n",
    "    print(country, country_entries_missing_totalcases, country_entries_missing_totalcasesmillion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results make sense and show that the 'per million' entries are probably deducted from the total numbers. This means the amount of missing entries per country is the same in those two columns. This means we only have to concentrate on the column total cases, to get a feedback on how many entries in general are missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to find out how many percentage of the data are missing in each country in the columns total cases and total deaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingdata_cases_nico(continent):\n",
    "    missing_data_caseslist_nico = []\n",
    "    \n",
    "    for country in locations_per_continent(continent):\n",
    "        country_cases_nico = df[df[columns_nico[2]] == country].iloc[:, 4]\n",
    "        country_casesmissing_nico = country_cases_nico.isnull().sum()\n",
    "        country_casesmissing_nico_percentage = (country_casesmissing_nico / len(country_cases_nico)) * 100\n",
    "        if country_casesmissing_nico_percentage == 100:\n",
    "            print('Hier fehlen 100% von den Daten: ' + country)\n",
    "        missing_data_caseslist_nico.append({'Country': country, 'Missing Entries': country_casesmissing_nico_percentage})\n",
    "        #print(f\"{country}: {sum_missing_entries_nico}\")\n",
    "    country_missingcases_df_nico = pd.DataFrame(missing_data_caseslist_nico)\n",
    "    #print(country_missingcases_df_nico)\n",
    "    \n",
    "    # Create countplot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Country', y='Missing Entries', data=country_missingcases_df_nico)\n",
    "    plt.title('Percentage of Missing Entries in Total Cases per Country in ' + continent)\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x labels for better readability\n",
    "    plt.xlabel('Country')\n",
    "    plt.ylabel('Percentage of Missing Entries in total cases')\n",
    "    plt.tight_layout()\n",
    "    safepoint = \"../output/Missing_Data_Analysis/cols0-16/missing_totalcases-\" + continent +\".png\"\n",
    "    plt.savefig(safepoint)\n",
    "    \n",
    "\n",
    "for continent in continents_of_interest:\n",
    "    missingdata_cases_nico(continent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all of these countries all entries in total cases are missing: Puerto Rico, Sint Maarten (Dutch part), United States Virgin Islands, Turkmenistan, Guernsey, Jersey, Western Sahara, Guam, Niue, Northern Mariana Islands, Pitcairn, Tokelau. I recommend to delete these countries.\n",
    "Now we will have a look at the column total deaths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingdata_deaths_nico(continent):\n",
    "    missing_data_deathslist_nico = []\n",
    "    \n",
    "    for country in locations_per_continent(continent):\n",
    "        country_deaths_nico = df[df[columns_nico[2]] == country].iloc[:, 7]\n",
    "        country_deathsmissing_nico = country_deaths_nico.isnull().sum()\n",
    "        country_deathsmissing_nico_percentage = (country_deathsmissing_nico / len(country_deaths_nico)) * 100\n",
    "        if country_deathsmissing_nico_percentage == 100:\n",
    "            print('Hier fehlen 100% von den Daten: ' + country)\n",
    "        missing_data_deathslist_nico.append({'Country': country, 'Missing Entries': country_deathsmissing_nico_percentage})\n",
    "        #print(f\"{country}: {sum_missing_entries_nico}\")\n",
    "    country_missingdeaths_df_nico = pd.DataFrame(missing_data_deathslist_nico)\n",
    "    #print(country_missingdeaths_df_nico)\n",
    "\n",
    "\n",
    "    # Create countplot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Country', y='Missing Entries', data=country_missingdeaths_df_nico)\n",
    "    plt.title('Percentage of Missing Entries in Total Deaths per Country in ' + continent)\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x labels for better readability\n",
    "    plt.xlabel('Country')\n",
    "    plt.ylabel('Percentage of Missing Entries in total deaths')\n",
    "    plt.tight_layout()\n",
    "    safepoint = \"../output/Missing_Data_Analysis/cols0-16/missing_totaldeaths-\" + continent +\".png\"\n",
    "    plt.savefig(safepoint)\n",
    "    \n",
    "\n",
    "for continent in continents_of_interest:\n",
    "    missingdata_deaths_nico(continent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countries in which all the entries in the column total deaths are missing are: Puerto Rico,Sint Maarten (Dutch part),United States Virgin Islands, Turkmenistan, Falkland Islands, Guernsey, Jersey, Vatican, Saint Helena, Western Sahara, Guam, Niue, Northern Mariana Islands, Pitcairn, Tokelau, Tuvalu. \n",
    "\n",
    "These contain also all the countries which were missing all entries in the column total cases. My recommendation is to delete these countries from the dataset.\n",
    "\n",
    "Finally we will have a look at the regions in the OWID part of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingdata_cases_nico_OWID():\n",
    "    missing_data_caseslist_OWID_nico = []\n",
    "    \n",
    "    for location in loc_OWID:\n",
    "        country_casesOWID_nico = df[df[columns_nico[2]] == location].iloc[:, 4]\n",
    "        country_casesmissingOWID_nico = country_casesOWID_nico.isnull().sum()\n",
    "        country_casesmissingOWID_nico_percentage = (country_casesmissingOWID_nico / len(country_casesOWID_nico)) * 100\n",
    "        if country_casesmissingOWID_nico_percentage == 100:\n",
    "            print('Hier fehlen 100% von den Daten: ' + location)\n",
    "        missing_data_caseslist_OWID_nico.append({'Country': location, 'Missing Entries': country_casesmissingOWID_nico_percentage})\n",
    "        #print(f\"{country}: {sum_missing_entries_nico}\")\n",
    "    country_missingcasesOWID_df_nico = pd.DataFrame(missing_data_caseslist_OWID_nico)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Country', y='Missing Entries', data=country_missingcasesOWID_df_nico)\n",
    "    plt.title('Percentage of Missing Entries in Total Cases per Location in OWID')\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x labels for better readability\n",
    "    plt.xlabel('Location')\n",
    "    plt.ylabel('Percentage of Missing Entries in total cases')\n",
    "    plt.tight_layout()\n",
    "    safepoint = \"../output/Missing_Data_Analysis/cols0-16/missing_totalcases-OWID.png\"\n",
    "    plt.savefig(safepoint)\n",
    "    \n",
    "\n",
    "missingdata_cases_nico_OWID()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingdata_deaths_nico_OWID():\n",
    "    missing_data_deathslist_OWID_nico = []\n",
    "    \n",
    "    for location in loc_OWID:\n",
    "        country_deathsOWID_nico = df[df[columns_nico[2]] == location].iloc[:, 7]\n",
    "        country_deathsmissingOWID_nico = country_deathsOWID_nico.isnull().sum()\n",
    "        country_deathsmissingOWID_nico_percentage = (country_deathsmissingOWID_nico / len(country_deathsOWID_nico)) * 100\n",
    "        if country_deathsmissingOWID_nico_percentage == 100:\n",
    "            print('Hier fehlen 100% von den Daten: ' + location)\n",
    "        missing_data_deathslist_OWID_nico.append({'Country': location, 'Missing Entries': country_deathsmissingOWID_nico_percentage})\n",
    "        #print(f\"{country}: {sum_missing_entries_nico}\")\n",
    "    country_missingdeathsOWID_df_nico = pd.DataFrame(missing_data_deathslist_OWID_nico)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Country', y='Missing Entries', data=country_missingdeathsOWID_df_nico)\n",
    "    plt.title('Percentage of Missing Entries in Total Deaths per Location in OWID')\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x labels for better readability\n",
    "    plt.xlabel('Location')\n",
    "    plt.ylabel('Percentage of Missing Entries in total deaths')\n",
    "    plt.tight_layout()\n",
    "    safepoint = \"../output/Missing_Data_Analysis/cols0-16/missing_totaldeaths-OWID.png\"\n",
    "    plt.savefig(safepoint)\n",
    "    \n",
    "\n",
    "missingdata_deaths_nico_OWID()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the regions England, Northern Ireland, Northern Cyprus, Scotland and Wales are missing all datapoints. I recommend to delete these regions too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Decision over keeping or deleting/imputing columns 16:32 (Leon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Missing data percentages (Leon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_leon = data.columns[16:32]\n",
    "missing_percentages_leon = df[columns_leon].isnull().sum() / df.shape[0] *100\n",
    "print(missing_percentages_leon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance it only makes sense to keep reproduction rate and maybe the ones with percentages of missing data below 75%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Visualization of variables of Interest (Leon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing all my variables for three countries to determine usefulness and worth of imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in columns_leon:    \n",
    "    belgium_data = df.groupby(\"iso_code\").get_group(\"BEL\")\n",
    "    belgium = belgium_data[column].fillna(0)\n",
    "\n",
    "    brazil_data = df.groupby(\"iso_code\").get_group(\"BRA\")\n",
    "    brazil = brazil_data[column].fillna(0)\n",
    "\n",
    "    mexico_data = df.groupby(\"iso_code\").get_group(\"MEX\")\n",
    "    mexico = mexico_data[column].fillna(0)\n",
    "\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(8, 6))\n",
    "\n",
    "    # Plot data on each subplot\n",
    "    axs[0].plot(belgium_data[\"date\"], belgium)\n",
    "    axs[0].set_xlabel(\"date\")\n",
    "    axs[0].set_ylabel(column)\n",
    "    axs[0].set_title(\"Belgium\")\n",
    "\n",
    "    axs[1].plot(brazil_data[\"date\"], brazil)\n",
    "    axs[1].set_xlabel(\"date\")\n",
    "    axs[1].set_ylabel(column)\n",
    "    axs[1].set_title(\"Brazil\")\n",
    "\n",
    "    axs[2].plot(mexico_data[\"date\"], mexico)\n",
    "    axs[2].set_xlabel(\"date\")\n",
    "    axs[2].set_ylabel(column)\n",
    "    axs[2].set_title(\"Mexico\")\n",
    "\n",
    "    # Adjust layout to prevent overlapping\n",
    "    plt.tight_layout()\n",
    "    safepoint = \"../output/Missing_Data_Analysis/cols16-32/missing_data-\" + column +\".png\"\n",
    "    plt.savefig(safepoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of them seem to be imputable. We will keep all variables for the cases since we will target number of cases in our prediction. The ones that were missing 75% plus of their values are also the ones that look like they will not be imputable by any means. Brazil also seems to be lacking positive rate completely (which was under 75% missing values but not in the cases group). Positive rate and reproduction rate are the only ones left to determine whether we will keep them or delete them (The ones with 75% plus missing will most likely be deleted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 How many countries are entirely missing my variables of interest? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_countries = list(df.groupby(\"iso_code\").groups.keys())\n",
    "for column in columns_leon:\n",
    "    grouped_by_iso = df.groupby(\"iso_code\")\n",
    "    nan_counts = grouped_by_iso[column].apply(lambda x: x.isnull().all()).sum()\n",
    "    print(\"Percentage of countries that completely miss all values of\", column,\":\", nan_counts/len(all_countries)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This again shows that the variables that miss more than 75% of their values are also very much the ones where the highest percentage of countries do not have any values of them stored at all. (90% means that 90 percent of all countries do not have a single value but Nan for this column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to keep all variables containing the tests, we would have to delete rows that contain only Nan for those columns. For what countries is that the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_variables = [var for var in columns_leon if 'test' in var]\n",
    "empty_countries_test = []\n",
    "for column in test_variables:  \n",
    "    test_nan_countries = grouped_by_iso[column].apply(lambda x: x.isnull().all())\n",
    "    test_nan_countries_true = test_nan_countries[test_nan_countries == True]\n",
    "    empty_countries_test.append(test_nan_countries_true.index)\n",
    "empty_countries_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what country is missing how many test variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_dict = {}\n",
    "for countries in empty_countries_test:\n",
    "    for country in countries:\n",
    "        existing_country = counter_dict.get(country)\n",
    "        if existing_country is not None:\n",
    "            counter_dict[country] += 1\n",
    "        else:\n",
    "            counter_dict[country] = 1\n",
    "sorted_counter_dict = dict(sorted(counter_dict.items(), key=lambda item:item[1]))\n",
    "print(sorted_counter_dict)\n",
    "sorted_counter_dict.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countries missing all 6 columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_without_tests = [key for key, value in counter_dict.items() if value == 6]\n",
    "countries_without_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to keep all test variables, we will have to delete all of those countries listed above. We will for sure delete the countries missing all 6 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Decision over keeping or deleting/imputing columns 32:48 (Florin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Decision over keeping or deleting/imputing columns 48:63 (Sevi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Rough overview over the percentages of missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_sevi = data.columns[48:63]\n",
    "missing_percentages_sevi = df[columns_sevi].isnull().sum() / df.shape[0] *100\n",
    "print(missing_percentages_sevi.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All  <75% that was previously defined -> no complete deletions\n",
    "\n",
    "7 columns are  <75% and >20% -> further investigation\n",
    "\n",
    "8 columns are <20% -> keep as is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Static Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first tried to analyze the change of the missing values over for a few countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in columns_sevi:    \n",
    "    belgium_data = df.groupby(\"iso_code\").get_group(\"BEL\")\n",
    "    belgium = belgium_data[column].fillna(0)\n",
    "\n",
    "    brazil_data = df.groupby(\"iso_code\").get_group(\"BRA\")\n",
    "    brazil = brazil_data[column].fillna(0)\n",
    "\n",
    "    mexico_data = df.groupby(\"iso_code\").get_group(\"MEX\")\n",
    "    mexico = mexico_data[column].fillna(0)\n",
    "\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(8, 6))\n",
    "\n",
    "    # Plot data on each subplot\n",
    "    axs[0].plot(belgium_data[\"date\"], belgium)\n",
    "    axs[0].set_xlabel(\"date\")\n",
    "    axs[0].set_ylabel(column)\n",
    "    axs[0].set_title(\"Belgium\")\n",
    "\n",
    "    axs[1].plot(brazil_data[\"date\"], brazil)\n",
    "    axs[1].set_xlabel(\"date\")\n",
    "    axs[1].set_ylabel(column)\n",
    "    axs[1].set_title(\"Brazil\")\n",
    "\n",
    "    axs[2].plot(mexico_data[\"date\"], mexico)\n",
    "    axs[2].set_xlabel(\"date\")\n",
    "    axs[2].set_ylabel(column)\n",
    "    axs[2].set_title(\"Mexico\")\n",
    "\n",
    "    # Adjust layout to prevent overlapping\n",
    "    plt.tight_layout()\n",
    "    safepoint = \"../output/Missing_Data_Analysis/cols48-63/missing_data-\" + column +\".png\"\n",
    "    plt.savefig(safepoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the number of values change over time.\n",
    "\n",
    "There is only one value for each country for the whole dataset.\n",
    "\n",
    "This may complicate any impution attempts\n",
    "\n",
    "Here a function to confirm this finding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_constant_values(data, country_col, time_col, variables):\n",
    "    results = {}\n",
    "    for country in data[country_col].unique():\n",
    "        country_data = data[data[country_col] == country]\n",
    "        results[country] = {}\n",
    "        for variable in variables:\n",
    "            unique_values = country_data[variable].dropna().unique()\n",
    "            results[country][variable] = len(unique_values) < 2\n",
    "    return results\n",
    "\n",
    "constant_value_check = check_constant_values(data_without_OWID, 'iso_code', 'date', columns_sevi)\n",
    "\n",
    "for country, variables in constant_value_check.items():\n",
    "    print(f\"Country: {country}\")\n",
    "    for var, is_constant in variables.items():\n",
    "        if is_constant == False:\n",
    "            print(f\"  {var}: varies\")\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Actual imputations and deletions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 First deletions that can already be made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More than 90% of entries are missing for those four variables:\n",
    "data_clean = data.drop([\"excess_mortality_cumulative_absolute\", \"excess_mortality_cumulative\", \"excess_mortality\", \"excess_mortality_cumulative_per_million\"], axis=1, inplace=False)\n",
    "\n",
    "\n",
    "#Country deletions (Needed to clean up Leon data): They miss all 6 test columns, we want to have at least one tests column for our algorithms:\n",
    "data_clean = data_clean[~data_clean['iso_code'].isin(countries_without_tests)]\n",
    "\n",
    "\n",
    "#Leons deletions of columns missing more than 75% of data overall and also over 80% of all countries miss those variables entirely :\n",
    "data_clean = data_clean.drop(['icu_patients', 'icu_patients_per_million', 'hosp_patients', 'hosp_patients_per_million', \n",
    "                              'weekly_icu_admissions', 'weekly_icu_admissions_per_million', 'weekly_hosp_admissions',\n",
    "                              'weekly_hosp_admissions_per_million'],axis=1, inplace=False)\n",
    "\n",
    "\n",
    "\n",
    "#insert all your drops and imputations here and save in data_clean!\n",
    "#Von jetzt an nur noch data_clean brauchen\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TO REMOVE ASWELL: ALL OWID (we already have created data frames that dont include OWID)\n",
    "data_clean = data_clean[~data_clean['iso_code'].str.contains('OWID')]\n",
    "\n",
    "\n",
    "print(\"Clean data columns left:\", data_clean.columns.to_list())\n",
    "print(\"Clean data countries left:\", data_clean[\"iso_code\"].unique())\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Data observation after first deletions and further deletions (Leons Columns, countries):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this to see how our data_clean behaves after certain removals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leon_columns_after_first_clean = [\"reproduction_rate\", \"total_tests\", \"new_tests\", \"total_tests_per_thousand\", \n",
    "                                  \"new_tests_per_thousand\", \"new_tests_smoothed\", \n",
    "                                  \"new_tests_smoothed_per_thousand\", \"positive_rate\"]\n",
    "\n",
    "all_countries = list(data_clean.groupby(\"iso_code\").groups.keys())\n",
    "for column in leon_columns_after_first_clean:\n",
    "    grouped_by_iso = data_clean.groupby(\"iso_code\")\n",
    "    nan_counts = grouped_by_iso[column].apply(lambda x: x.isnull().all()).sum()\n",
    "    print(\"Percentage of countries that completely miss all values of\", column,\":\", nan_counts/len(all_countries)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have 19% of the countries that are entirely missing new_tests and new_tests per thousand. Those will need to be removed, after that we still have 4 variables that contain information about tests, which should be enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data_clean.drop([\"new_tests\", \"new_tests_per_thousand\"],axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing what countries are still missing positive_rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_countries_p_rate = []\n",
    "p_rate_loc = grouped_by_iso[\"positive_rate\"].apply(lambda x: x.isnull().all())\n",
    "p_rate_nan_countries_true = p_rate_loc[p_rate_loc == True]\n",
    "empty_countries_p_rate.append(p_rate_nan_countries_true.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting those countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data_clean[~data_clean['iso_code'].isin(empty_countries_p_rate[0].tolist())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing what countries are still missing total_tests_per_thousand: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_countries_ttt = []\n",
    "ttt_loc = grouped_by_iso[\"total_tests_per_thousand\"].apply(lambda x: x.isnull().all())\n",
    "ttt_nan_countries_true = ttt_loc[ttt_loc == True]\n",
    "empty_countries_ttt.append(ttt_nan_countries_true.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting those countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data_clean[~data_clean['iso_code'].isin(empty_countries_ttt[0].tolist())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing what countries are still missing reproduction_rate: (Also heavily affects Nicos data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_countries_r_rate = []\n",
    "r_rate_loc = grouped_by_iso[\"reproduction_rate\"].apply(lambda x: x.isnull().all())\n",
    "r_rate_nan_countries_true = r_rate_loc[r_rate_loc == True]\n",
    "empty_countries_r_rate.append(r_rate_nan_countries_true.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting those countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data_clean[~data_clean['iso_code'].isin(empty_countries_r_rate[0].tolist())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation after further deletions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_countries = list(data_clean.groupby(\"iso_code\").groups.keys())\n",
    "for column in data_clean.columns:\n",
    "    grouped_by_iso = data_clean.groupby(\"iso_code\")\n",
    "    nan_counts = grouped_by_iso[column].apply(lambda x: x.isnull().all()).sum()\n",
    "    print(\"Percentage of countries that completely miss all values of\", column,\":\", nan_counts/len(all_countries)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What countries are left after deletions (Leon):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_left = data_clean.groupby(\"iso_code\").groups.keys()\n",
    "print(countries_left)\n",
    "print(len(countries_left))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My remaining six columns have no more completely empty rows. Now on to the imputation of those columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 Imputations of remaining six columns (Leon):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To impute (Leon): reproduction_rate, total_tests, total_tests_per_thousand, new_tests_smoothed, new_tests_smoothed_per_thousand, positive_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing data percentages for the columns are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leon_columns_after_second_clean =[\"reproduction_rate\", \"total_tests\", \"total_tests_per_thousand\", \n",
    "                                  \"new_tests_smoothed\", \n",
    "                                  \"new_tests_smoothed_per_thousand\", \"positive_rate\"]\n",
    "missing_percentages_leon_after_del = data_clean[leon_columns_after_second_clean].isnull().sum() / data_clean.shape[0] *100\n",
    "print(missing_percentages_leon_after_del)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may seem like a lot for the latter 5 but if we consider that those columns have a lot of those entries missing at the start or the end they are easily imputable. They miss at start and end because countries either have not tracked them from the beginning or have stopped tracking after a certain time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN imputation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data_clean.interpolate(method='linear', subset=leon_columns_after_second_clean)\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "data_clean[leon_columns_after_second_clean] = knn_imputer.fit_transform(data_clean[leon_columns_after_second_clean])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had to first do linear imputations for the end and start parts. There was still missing data after that, thats why there is also the knn imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at how imputation went:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_percentages_leon_after_imp = data_clean[leon_columns_after_second_clean].isnull().sum() / data_clean.shape[0] *100\n",
    "print(missing_percentages_leon_after_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those columns have no more missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in leon_columns_after_second_clean:    \n",
    "    belgium_data = data_clean.groupby(\"iso_code\").get_group(\"BEL\")\n",
    "    belgium = belgium_data[column]\n",
    "\n",
    "    mexico_data = data_clean.groupby(\"iso_code\").get_group(\"MEX\")\n",
    "    mexico = mexico_data[column]\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(8, 5))\n",
    "\n",
    "    # Plot data on each subplot\n",
    "    axs[0].plot(belgium_data[\"date\"], belgium)\n",
    "    axs[0].set_xlabel(\"date\")\n",
    "    axs[0].set_ylabel(column)\n",
    "    axs[0].set_title(\"Belgium\")\n",
    "\n",
    "    axs[1].plot(mexico_data[\"date\"], mexico)\n",
    "    axs[1].set_xlabel(\"date\")\n",
    "    axs[1].set_ylabel(column)\n",
    "    axs[1].set_title(\"Mexico\")\n",
    "\n",
    "    # Adjust layout to prevent overlapping\n",
    "    plt.tight_layout()\n",
    "    safepoint = \"../output/Missing_Data_Analysis/cols16-32/missing_data-\" + column +\"-after_imputation.png\"\n",
    "    plt.savefig(safepoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4 Imputations of columns (Sevi):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-KNN imputation works well for all seven variables, because they are all descriptive variables for a country's socioeconomic state.\n",
    "\n",
    "-It identifies similarities based on multiple factors like health, economy, and demographics, ensuring the imputed values fit well with existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "variables_to_impute = ['handwashing_facilities', 'extreme_poverty', 'male_smokers', \n",
    "                       'female_smokers', 'hospital_beds_per_thousand', 'human_development_index',\n",
    "                       'aged_65_older']\n",
    "data_clean[variables_to_impute] = knn_imputer.fit_transform(data_clean[variables_to_impute])\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Data preprocessing (Handling of missing data excluded) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure\n",
    "1. Looking for outliers\n",
    "2. Distribution of data\n",
    "3. Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Looking for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "summary_stats = data_clean.describe()\n",
    "Q1 = summary_stats.loc['25%']\n",
    "Q3 = summary_stats.loc['75%']\n",
    "IQR = Q3 - Q1\n",
    "# Define outliers using IQR rule\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "# Align DataFrame columns with summary statistics columns\n",
    "data_clean, lower_bound = data_clean.align(lower_bound, axis=1, join='inner')\n",
    "data_clean, upper_bound = data_clean.align(upper_bound, axis=1, join='inner')\n",
    "outliers = data_clean[(data_clean < lower_bound) | (data_clean > upper_bound)].dropna(axis=1, how='all')\n",
    "if not outliers.empty:\n",
    "    print(\"Outliers:\")\n",
    "    print(outliers)\n",
    "else:\n",
    "    print(\"No outliers found.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Distribution of data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Distribution of numercial data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing whether the data is normally distributed or not:\n",
    "Using the anderson darling test, since the shapiro wilks is not suitable for data n > 5000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data_clean = data_clean.select_dtypes(include='number')\n",
    "distribution_results = {}\n",
    "for column in num_data_clean.columns:\n",
    "    result = sts.anderson(num_data_clean[column].dropna(), dist='norm')  ############################### I had to drop all na for it to work!!! -> maybe imputation needed\n",
    "    test_stat = result.statistic \n",
    "    critical_val = result.critical_values\n",
    "    #print(critical_val)                                \n",
    "    #print(test_stat)                           \n",
    "    if test_stat > critical_val[2]:\n",
    "        result = \"not normal\"\n",
    "    else:\n",
    "        result = \"normal\"    \n",
    "    distribution_results[column] = result\n",
    "\n",
    "\n",
    "not_normal = []\n",
    "for key, value in distribution_results.items():\n",
    "    if value == \"not normal\":\n",
    "        not_normal.append(key)\n",
    "if len(not_normal) == 0:\n",
    "    print(\"All numerical data seems to be normally distributed.\")\n",
    "elif len(not_normal) == len(num_data_clean.columns):\n",
    "    print(\"No variable is normally distributed.\")\n",
    "else:\n",
    "    print(\"All data but\", not_normal, \"seems to be normally distributed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of numercial data visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_per_row = 3\n",
    "num_columns = len(num_data_clean.columns)\n",
    "num_rows = (num_columns - 1) // columns_per_row + 1\n",
    "fig_width = 6 * columns_per_row\n",
    "fig_height = 4 * num_rows\n",
    "fig, axs = plt.subplots(num_rows, 4, figsize=(fig_width, fig_height))\n",
    "axs = axs.flatten()\n",
    "for i, column in enumerate(num_data_clean.columns):\n",
    "    ax = axs[i]\n",
    "    ax.hist(num_data_clean[column], bins=35, color='skyblue', edgecolor='black')\n",
    "    ax.set_title(column)\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../output/Preprocessing/Distributions/distributions_of_variables.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Total Deaths and Total Cases are both numerical values, therefore mainly regression is suitable, regression models that could be implemented are:\n",
    "\n",
    "1) **traditional linear regression (lasso/ridge)**\n",
    "\n",
    "- *Simplicity and Interpretability*: These models are straightforward to implement and interpret, making it easy to understand the relationship between predictors and the target variables.\n",
    "\n",
    "- *Handling Multicollinearity*: Lasso and Ridge can manage multicollinearity among features, improving the model's stability and performance.\n",
    "    \n",
    "- *Feature Selection*: Lasso performs automatic feature selection by shrinking some coefficients to zero, helping identify the most significant predictors.\n",
    "\n",
    "2) **random forest regression**\n",
    "\n",
    "- *Non-Linear Relationships:* This model captures complex non-linear interactions between features, which is crucial for accurately modeling real-world phenomena like the spread of a virus.\n",
    "\n",
    "- *Robustness and Generalization:* By averaging the predictions of multiple decision trees, Random Forests reduce overfitting and provide robust predictions.\n",
    "\n",
    "- *Feature Importance:* It offers insights into feature importance, helping to understand which factors most influence total cases and deaths.\n",
    "\n",
    "3) **support vector regression**\n",
    "\n",
    "- *Handling Non-Linearities:* SVR can model complex relationships using various kernels, making it suitable for datasets with non-linear patterns.\n",
    "\n",
    "- *Margin Maximization:* It focuses on minimizing prediction error within a margin, providing a balance between accuracy and generalization.\n",
    "\n",
    "- *Effective in High-Dimensional Spaces:* SVR performs well with high-dimensional data, which can be beneficial when dealing with multiple predictors for total deaths and total cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Traditional Regression Analysis\n",
    "\n",
    "- Use multiple linear regression or other suitable regression techniques.\n",
    "\n",
    "- Examine the relationship between total_cases/total_deaths and various predictors like:\n",
    "\n",
    "    -stringency index\n",
    "\n",
    "    -vaccination rates\n",
    "\n",
    "    -demographic factors\n",
    "\n",
    "    -regionality\n",
    "\n",
    "2) Short-Term Predictions\n",
    "\n",
    "- Use time-series forecasting techniques such as ARIMA, Prophet, or other regression-based forecasting models.\n",
    "\n",
    "- Make short-term predictions for total_cases and total_deaths by cutting out recent time periods.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
