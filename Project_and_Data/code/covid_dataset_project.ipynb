{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FoDS Project: Covid "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure\n",
    "1. Environment\n",
    "2. Data import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Environment\n",
    "1. python == 3.11.8 \n",
    "2. matplotlib=3.8.3\n",
    "3. numpy=1.26.0\n",
    "4. pandas=2.1.1\n",
    "5. scipy=1.12.0\n",
    "6. seaborn=0.13.2\n",
    "7. sklearn=1.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as sts\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.impute import KNNImputer\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/OWID-covid-data-28Feb2023.csv\")\n",
    "df = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Getting an overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure\n",
    "1. Basic overview\n",
    "2. Datatypes\n",
    "3. Locations and continents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of observations:\", data.shape[0])\n",
    "print(\"Number of columns:\", data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Datatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 All datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datatypes of each variable:\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_data = data.select_dtypes(include=(\"int64\")).columns\n",
    "print(\"INTEGERS: \", int_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we do not have any integers in our data. (We have but they are just not saved as such)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_data = data.select_dtypes(include=(\"float64\")).columns\n",
    "print(\"FLOATS: \", float_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_data = data.select_dtypes(include=(\"object\")).columns\n",
    "print(\"OBJECTS:\", object_data)\n",
    "num_data = data[float_data] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Saving numerical and categorical data in seperate data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = data[float_data] \n",
    "cat_data = data[object_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Adjusting date datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"date\"] = pd.to_datetime(data[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Introductory Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure\n",
    "1. Cases Visualized\n",
    "2. Deaths Visualized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cases Visualized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Cases per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: Most countries did not publish covid numbers on the weekends\n",
    "# Effect: This leads to an oscillating graph for daily new cases\n",
    "# This can be solved with weekly new cases\n",
    "# either extract data from new_daily or use the smoothed cases count\n",
    "daily_cases = data.groupby('date')['new_cases'].sum()\n",
    "data_march= data[(data['date']>'2022-03-01')&(data['date']<'2022-04-01')]\n",
    "daily_cases_march = data_march.groupby('date')['new_cases'].sum()\n",
    "daily_cases_weekly= data.groupby(pd.Grouper(key='date', freq='W')).sum()['new_cases']\n",
    "\n",
    "# Plotting\n",
    "fig, (ax1,ax2,ax3, ax4)=plt.subplots(4,1,figsize=(10,20))\n",
    "ax1.plot(daily_cases, label='New Cases Per Day')\n",
    "ax1.set(title='COVID-19 New Cases Per Day',\n",
    "        xlabel='Date',\n",
    "        ylabel='Number of New Cases'\n",
    "        )\n",
    "ax1.legend()\n",
    "ax2.plot(daily_cases_march, label='New Cases Per Day, March 2022')\n",
    "ax2.set(title='COVID-19 New Cases Per Day',\n",
    "        xlabel='Date',\n",
    "        ylabel='Number of New Cases'\n",
    "        )\n",
    "ax2.legend()\n",
    "ax3.plot(daily_cases_weekly, label='New Cases Per Day, Daily turned to weekly')\n",
    "ax3.set(title='COVID-19 New Cases Per Day',\n",
    "        xlabel='Date',\n",
    "        ylabel='Number of New Cases'\n",
    "        )\n",
    "\n",
    "ax3.legend()\n",
    "ax4.plot(data.groupby('date')['new_cases_smoothed'].sum(), label='New Cases Per Day, Daily turned to weekly')\n",
    "ax4.set(title='COVID-19 New Cases Per Day',\n",
    "        xlabel='Date',\n",
    "        ylabel='Number of New Cases'\n",
    "        )\n",
    "\n",
    "ax4.legend()\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.savefig('../output/Introductory_Visualizations/Cases/newcasestotal.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Cases over time by continent compact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data[~(data['continent']==0)].groupby(['date', 'continent'])['new_cases_smoothed'].sum().reset_index()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.lineplot(x='date', y='new_cases_smoothed', hue='continent', data=grouped_data)\n",
    "plt.title('New COVID-19 Cases by Continent Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of New Cases')\n",
    "plt.legend(title='Continent')\n",
    "plt.grid(True)  \n",
    "plt.savefig('../output/Introductory_Visualizations/Cases/newcasesbycontinent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Cases over time by continent in subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data[~(data['continent']==0)].groupby(['continent','date'])['new_cases_smoothed'].sum().reset_index()\n",
    "continents_of_interest=['North America','South America','Asia','Europe','Oceania','Africa']\n",
    "fig, axs = plt.subplots(1,6,figsize=(24,8), sharey=True)\n",
    "for i,continent in enumerate(continents_of_interest):\n",
    "        sns.lineplot(x='date', y='new_cases_smoothed',  data=grouped_data[grouped_data['continent']==continent],ax=axs[i])\n",
    "        axs[i].set_xlabel('Date')\n",
    "        axs[i].set_ylabel('New Cases')\n",
    "        axs[i].set_title(continent)\n",
    "        axs[i].tick_params(axis='x', rotation=90)\n",
    "        axs[i].grid(True)\n",
    "plt.suptitle('New Cases by Continent')\n",
    "plt.savefig('../output/Introductory_Visualizations/Cases/casesbycontinentsp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Total cases vs total cases per million by continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases=['total_cases','total_cases_per_million']\n",
    "fig, axs= plt.subplots(2,1,figsize=(10,10))\n",
    "data_nozero=data[~(data['continent']==0)]\n",
    "for i, variable in enumerate(cases):\n",
    "        top_countries = data_nozero.groupby('continent')[variable].max().nlargest(10)\n",
    "        print(top_countries)\n",
    "        top_countries.plot(kind='bar', ax=axs[i],grid=True)\n",
    "        axs[i].set_xlabel('Continent')\n",
    "        axs[i].set_ylabel(variable)\n",
    "plt.suptitle('Total Cases vs Total Cases per Million')\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.savefig('../output/Introductory_Visualizations/Cases/casesbycontinent.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Total cases vs total cases per million by country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases=['total_cases','total_cases_per_million']\n",
    "fig, axs= plt.subplots(2,1,figsize=(10,10))\n",
    "data_noowid=data[~data['iso_code'].str.startswith(\"OWID_\")]\n",
    "for i, variable in enumerate(cases):\n",
    "        top_countries = data_noowid.groupby('iso_code')[variable].max().nlargest(10)\n",
    "        print(top_countries)\n",
    "        top_countries.plot(kind='bar', ax=axs[i],grid=True)\n",
    "        axs[i].set_xlabel('Country')\n",
    "        axs[i].set_ylabel(variable)\n",
    "plt.suptitle('Total Cases vs Total Cases per Million')\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.savefig('../output/Introductory_Visualizations/Cases/casesbycountry.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Scatter plots with variables of interest to see correlation with total cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_of_interest = [\n",
    "    'icu_patients_per_million', 'total_tests_per_thousand', 'total_vaccinations_per_hundred',\n",
    "    'stringency_index', 'hospital_beds_per_thousand', 'aged_65_older'\n",
    "]\n",
    "fig, axs = plt.subplots(3, 2, figsize=(15, 15))\n",
    "axs = axs.flatten()\n",
    "for i, variable in enumerate(variables_of_interest):\n",
    "    sns.scatterplot(x=variable, y='total_cases_per_million', data=data, ax=axs[i], markers='.')\n",
    "    axs[i].set_title(f'Total Cases per Million vs. {variable}')\n",
    "    axs[i].set_xlabel(variable)\n",
    "    axs[i].set_ylabel('Total Cases per Million')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/Introductory_Visualizations/Cases/totcasescorr.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Deaths visualized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 New deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_deaths = data.groupby('date')['new_deaths'].sum()\n",
    "daily_deaths_smoothed = data.groupby('date')['new_deaths_smoothed'].sum()\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(2,1,figsize=(10,20))\n",
    "\n",
    "ax1.plot(daily_deaths, label='New Deaths Per Day')\n",
    "ax1.set(title='COVID-19 New Deaths Per Day', xlabel='Date', ylabel='Number of New Deaths')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(daily_deaths_smoothed, label='New Deaths Per Day, Smoothed Data')\n",
    "ax2.set(title='COVID-19 New Deaths Per Day', xlabel='Date', ylabel='Number of New Deaths')\n",
    "ax2.legend()\n",
    "\n",
    "plt.savefig(\"../output/Introductory_Visualizations/Deaths/new_deaths.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to find out if there is a difference in deaths per million among the continents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_deaths_per_million_continent = data.groupby(['date', 'continent'])['new_deaths_per_million'].sum().reset_index()\n",
    "new_deaths_per_million_continent_smoothed = data.groupby(['date', 'continent'])['new_deaths_smoothed_per_million'].sum().reset_index()\n",
    "total_cases_per_continent = data.groupby(['date', 'continent'])['total_cases_per_million'].sum().reset_index()\n",
    "print(new_deaths_per_million_continent)\n",
    "print(new_deaths_per_million_continent_smoothed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 New deaths per million by continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continents_deaths = new_deaths_per_million_continent['continent'].unique()\n",
    "#Creating suplots per Continent with the real counts\n",
    "fig, axes = plt.subplots(len(continents_deaths), 1, figsize=(10, 6*len(continents_deaths)), sharex=True)\n",
    "# Iterate over each continent and create a subplot\n",
    "for i, continent in enumerate(continents_deaths):\n",
    "    continent_data = new_deaths_per_million_continent[new_deaths_per_million_continent['continent'] == continent]\n",
    "    sns.lineplot(data=continent_data, x='date', y='new_deaths_per_million', ax=axes[i])\n",
    "    axes[i].set_title(f'New Deaths per Million in {continent} Over Time')\n",
    "    axes[i].set_ylabel('New Deaths per Million')\n",
    "    axes[i].set_xlabel('Date')\n",
    "    axes[i].grid(True)\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"../output/Introductory_Visualizations/Deaths/NewDeathsPerMillion_Subplots.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Smoothed new deaths per million by continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(continents_deaths), 1, figsize=(10, 6*len(continents_deaths)), sharex=True)\n",
    "\n",
    "for i, continent in enumerate(continents_deaths):\n",
    "    continent_data = new_deaths_per_million_continent_smoothed[new_deaths_per_million_continent_smoothed['continent'] == continent]\n",
    "    sns.lineplot(data=continent_data, x='date', y='new_deaths_smoothed_per_million', ax=axes[i])\n",
    "    axes[i].set_title(f'New Deaths per Million in {continent} Over Time')\n",
    "    axes[i].set_ylabel('New Deaths per Million')\n",
    "    axes[i].set_xlabel('Date')\n",
    "    axes[i].grid(True)\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"../output/Introductory_Visualizations/Deaths/NewDeathsPerMillionSmoothed_Subplots.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 New deaths per million divided by total cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to make a statement about the severity by analyzing the new deaths per million divided by the total cases.\n",
    "Merge the new deaths and total cases. Calculate new deaths per million divided by total cases per million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_cases_merge = pd.merge(new_deaths_per_million_continent_smoothed, total_cases_per_continent, on=['date', 'continent'])\n",
    "death_cases_merge['deaths_to_cases_ratio'] = death_cases_merge['new_deaths_smoothed_per_million'] / death_cases_merge['total_cases_per_million']\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(len(continents_deaths), 1, figsize=(10, 6*len(continents_deaths)), sharex=True)\n",
    "\n",
    "for i, continent in enumerate(continents_deaths):\n",
    "    continent_data = death_cases_merge[death_cases_merge['continent'] == continent]\n",
    "    sns.lineplot(data=continent_data, x='date', y='deaths_to_cases_ratio', ax=axes[i])\n",
    "    axes[i].set_title(f'New Deaths per Million divided by Total Cases per Million in {continent} Over Time')\n",
    "    axes[i].set_ylabel('New Deaths per Million / Total Cases per Million')\n",
    "    axes[i].set_xlabel('Date')\n",
    "    axes[i].grid(True)\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"../output/Introductory_Visualizations/Deaths/NewDeathsToCasesRatio_Subplots.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Missing Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introductory remarks about OWID\n",
    "2. Preparatory steps\n",
    "3. First overview of missing data\n",
    "4. Overview missing data \"cases\" and \"deaths\"\n",
    "5. Total cases continent analysis\n",
    "6. Factual accuracy\n",
    "7. Country analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introductory remarks about OWID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OWID itself states that it generally takes its data from 4 different sources:\n",
    "1. Specialized institutes\n",
    "2. Research articles\n",
    "3. International Institutations or statistics agencies\n",
    "4. Official data from government sources\n",
    "\n",
    "More specifically, looking at the OWID COVID dataset online where all the sources are indicated, it can be seen that OWID takes most of the data directly from the WHO COVID dashboard and then completes it with other sources which is why this dataset can be expected to be quite complete. However, the following analysis is attempting to analyze how much data is missing from the dataset and what kind of data is missing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preparatory steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Adjusting visualization settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colorblind-friendly palette\n",
    "color_palette = sns.color_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjusting the viewing options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicates\n",
    "data = data.drop_duplicates()\n",
    "#separating OWID from non_OWID data\n",
    "data_without_OWID = data[~data['iso_code'].str.contains('OWID')]\n",
    "data_OWID = data[data['iso_code'].str.contains('OWID')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What OWID contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_OWID_loc = data_OWID.groupby(\"location\")\n",
    "loc_OWID = list(grouped_OWID_loc.groups.keys())\n",
    "print(loc_OWID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OWID contains a few extra metrics and countries/continents or composites of such."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. First overview of missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Entries with complete information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For how many entries do we have complete information?\n",
    "(data.isna().sum(axis=1) == 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Missing values per column overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing values overall per column\n",
    "total_missing = data.isnull().sum()\n",
    "total_missing_percentage = (total_missing / len(data)) * 100\n",
    "total_missing_sorted = total_missing.sort_values()\n",
    "total_missing_percentage_sorted = total_missing_percentage.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graphing the missing values per column\n",
    "plt.figure(figsize=(6, 8))\n",
    "fig_mv_variables = sns.barplot(\n",
    "    x = total_missing_percentage_sorted.values,\n",
    "    y = total_missing_percentage_sorted.index\n",
    ")\n",
    "fig_mv_variables.set(\n",
    "    title=\"Missing values per variable\",\n",
    "    xlabel=\"Percentage of Missing Values\",\n",
    "    ylabel=\"Variables\"\n",
    ")\n",
    "plt.savefig(\"../output/Missing_Data_Analysis/missing_data.png\", bbox_inches=\"tight\")              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Overview missing data \"cases\" and \"deaths\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Selecting only the columns containing information about cases and deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting the columns about cases of interest\n",
    "columns_about_cases = data.loc[:, data.columns.str.contains('cases')]\n",
    "columns_about_cases.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_about_deaths = data.loc[:, data.columns.str.contains('death')]\n",
    "columns_about_deaths.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Sorting and plotting overall missing data about cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_data_filtered(columns_of_interest):\n",
    "    total_missing = columns_of_interest.isnull().sum()\n",
    "    total_missing_percentage = (total_missing / len(data)) * 100\n",
    "    total_missing_sorted = total_missing.sort_values()\n",
    "    total_missing_percentage_sorted = total_missing_percentage.sort_values()\n",
    "    return total_missing_percentage_sorted\n",
    "\n",
    "total_missing_percentage_deaths_sorted = missing_data_filtered(columns_about_deaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "deaths_overview = sns.barplot(\n",
    "    x = total_missing_percentage_deaths_sorted.values,\n",
    "    y = total_missing_percentage_deaths_sorted.index\n",
    ")\n",
    "deaths_overview.set(\n",
    "    title=\"Missing values per variable\",\n",
    "    xlabel=\"Percentage of Missing Values\",\n",
    "    ylabel=\"Variables\"\n",
    ")\n",
    "plt.savefig(\"../output/Missing_Data_Analysis/deathsmissing_data.png\", bbox_inches=\"tight\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Sorting and plotting overall missing data about deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_missing_percentage_cases_sorted = missing_data_filtered(columns_about_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "cases_overview = sns.barplot(\n",
    "    x = total_missing_percentage_cases_sorted.values,\n",
    "    y = total_missing_percentage_cases_sorted.index\n",
    ")\n",
    "cases_overview.set(\n",
    "    title=\"Missing values per variable\",\n",
    "    xlabel=\"Percentage of Missing Values\",\n",
    "    ylabel=\"Variables\"\n",
    ")\n",
    "plt.savefig(\"../output/Missing_Data_Analysis/casesmissing_data.png\", bbox_inches=\"tight\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is in the overall dataset a lot more data around cases than deaths. However, amongst the columns that give information about either death or cases, the variability as to the amount of missing data is low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Total cases continent analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Amount of data per country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: Since reporting is generally done on a national level, it makes sense to have a look at individual countries to find out where potential data is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many countries are we dealing with overall\n",
    "unique_values_world = data_without_OWID['iso_code'].unique()\n",
    "print (len(unique_values_world))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the number of entries for each country\n",
    "length_list = []\n",
    "length_list_in_years = []\n",
    "\n",
    "for i in range(len(unique_values_world)):\n",
    "    country = unique_values_world[i]\n",
    "    length_country = len(data_without_OWID.loc[df['iso_code'] == country])\n",
    "    length_list.append(length_country)\n",
    "    length_list_in_years.append(length_country/365)\n",
    "\n",
    "sorted_country_length = sorted(length_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(length_list_in_years, bins=30, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('data points converted to number of years')\n",
    "plt.ylabel('number of countries')\n",
    "plt.title(\"Number of entries in the data set per country\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Total cases Asia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different continents\n",
    "filtered_continents = data_without_OWID[\"continent\"].unique()\n",
    "filtered_continents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function returning list of all different countries on one continent\n",
    "def countries_per_continent(continent):\n",
    "    filtered_continent = data_without_OWID.loc[df['continent'] == continent]\n",
    "    unique_values_continent = filtered_continent['iso_code'].unique()\n",
    "    return unique_values_continent\n",
    "\n",
    "unique_values_Asia = countries_per_continent(\"Asia\")\n",
    "unique_values_Asia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continent_values(continent_uniq_val):\n",
    "    values = []\n",
    "    for i in range(len(continent_uniq_val)):\n",
    "        currentcountry = continent_uniq_val[i]\n",
    "        filtered_country = data.loc[df['iso_code'] == currentcountry]\n",
    "        no_rows_current_country = len(filtered_country)\n",
    "        missing = (filtered_country[\"total_cases\"].isnull().sum())\n",
    "        total_missing_percentage_current_country = (missing / len(filtered_country)) * 100\n",
    "        values.append(total_missing_percentage_current_country)\n",
    "    return values\n",
    "\n",
    "values_Asia = continent_values(unique_values_Asia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_barplot_continent (unique_codes, values):\n",
    "    plt.figure(figsize=(26, 8))\n",
    "    missing_values_Asia = sns.barplot(\n",
    "        x = unique_codes,\n",
    "        y = values\n",
    "    )\n",
    "\n",
    "    deaths_overview.set(\n",
    "        title=\"Missing values per variable\",\n",
    "        xlabel=\"Percentage of Missing Values\",\n",
    "        ylabel=\"Variables\"\n",
    "    )\n",
    "\n",
    "    plt.savefig(\"../output/Missing_Data_Analysis/casesmissing_data2.png\", bbox_inches=\"tight\") \n",
    "\n",
    "plotting_barplot_continent(unique_values_Asia, values_Asia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Total Cases Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values_Europe = countries_per_continent(\"Europe\")\n",
    "unique_values_Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_Europe = continent_values(unique_values_Europe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_barplot_continent(unique_values_Europe, values_Europe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Total Cases Africa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values_Africa = countries_per_continent(\"Africa\")\n",
    "unique_values_Africa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_Africa = continent_values(unique_values_Africa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_barplot_continent(unique_values_Africa, values_Africa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5 Total Cases North America"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values_North_America = countries_per_continent(\"North America\")\n",
    "unique_values_North_America"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_North_America = continent_values(unique_values_North_America)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_barplot_continent(unique_values_North_America, values_North_America)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Factual accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Do total cases and new cases lead to the same numbers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at Canada the reporting started on 23 of January but the first death was on March 9. A quick search on the internet confirms that 9 March was the date on which the frist Canadian person died. That makes 46 days on which there is no data on deaths. But there are 1132 entries for Canada and hence 46/1132 = 4%. In the case of Switzerland 10 no values for deaths and 1099 rows so that would make 1% of the data missing. For Germany 42/1128 = 4%. All of this data is not missing, it is just wrongly classified as NA when these should be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_total_new_cases(country): \n",
    "    filtered_country = data_without_OWID.loc[df[\"location\"] == \"Sweden\"]\n",
    "    filtered_country_total_cases_from_tc = filtered_country.iloc[-1, 4]\n",
    "    print (\"total cases from the total cases column: \", filtered_country_total_cases_from_tc)\n",
    "    print (\"total cases when summing over new cases: \", filtered_country[\"new_cases\"].sum())\n",
    "\n",
    "comparison_total_new_cases(\"Sweden\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 How accurate are vaccination statistics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Wikipedia, the first vaccination in Switzerland, Germany and France tool place on December 23 2020, December 22, 2020 and December 17, 2020 respectively. The dataset confirms these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_vaccination_statistics_accuracy(country):\n",
    "    filtered_country = data_without_OWID.loc[df['location'] == country]\n",
    "    filtered_country = filtered_country.reset_index(drop=True)\n",
    "    first_vaccination_index = filtered_country[\"total_vaccinations\"].first_valid_index()\n",
    "    return filtered_country.iloc[first_vaccination_index,:5]\n",
    "    \n",
    "first_vaccination_statistics_accuracy(\"France\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Country analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Function to replace missing data with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here is how to replace the mssing values with zero values\n",
    "def replace_missing_data(country):\n",
    "    filtered_country = data_without_OWID.loc[df['location'] == country]\n",
    "    #filtered_country = filtered_Italy.reset_index(drop=True)\n",
    "    missing_vals = [\"NA\", \"\", None, np.NaN]\n",
    "    missing_country = filtered_country.isin(missing_vals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Function for missing values excluding missing at the end and at the beginning for a specific country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering_missing_country(country):\n",
    "    filtered_country = data_without_OWID.loc[df['location'] == country]\n",
    "    filtered_country = filtered_country.reset_index(drop=True)\n",
    "    filtered_country_summary = filtered_country.isnull().sum()\n",
    "    first_valid_indices_country = filtered_country.apply(lambda x: x.first_valid_index())\n",
    "    last_valid_indices_country = filtered_country.apply(lambda x: x.last_valid_index())\n",
    "    length = len(filtered_country) - 1\n",
    "\n",
    "    filtered_country_summary_df = filtered_country_summary.to_frame()\n",
    "    first_valid_indices_country_df = first_valid_indices_country.to_frame()\n",
    "    last_valid_indices_country_df = last_valid_indices_country.to_frame()\n",
    "    list_of_dataframes = [filtered_country_summary_df, first_valid_indices_country_df, last_valid_indices_country]\n",
    "\n",
    "    comparison_country = pd.merge(filtered_country_summary_df, first_valid_indices_country_df, left_index=True, right_index=True)\n",
    "    comparison_country = comparison_country.rename(columns={\"0_x\": '# of missing entries', \"0_y\": \"first entry\"})\n",
    "    diff_first = filtered_country_summary_df - first_valid_indices_country_df\n",
    "    last_valid_indices_country_df_diff = length - last_valid_indices_country_df\n",
    "    diff_overall = filtered_country_summary_df - first_valid_indices_country_df - last_valid_indices_country_df_diff\n",
    "    comparison_country = comparison_country.assign(diff_first = diff_first, \\\n",
    "                last_entry= last_valid_indices_country_df, diff_last = last_valid_indices_country_df_diff, \\\n",
    "                diff_overall = diff_overall)\n",
    "    return (comparison_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering_missing_country(\"Finland\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 Function total diff for many countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locations_per_continent(continent):\n",
    "    filtered_continent = data_without_OWID.loc[df['continent'] == continent]\n",
    "    unique_values_continent = filtered_continent['location'].unique()\n",
    "    return unique_values_continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_locations_Europe = locations_per_continent(\"Europe\")\n",
    "empty_df = pd.DataFrame()\n",
    "\n",
    "def several_countries(list, dataframe):\n",
    "    count = 0\n",
    "    for i in (list):\n",
    "        country = filtering_missing_country(i)\n",
    "        #print (country)\n",
    "        country_diff_overall_column = country[\"diff_overall\"]\n",
    "        #print (country_diff_overall_column)\n",
    "        country_diff_overall_column_df = country_diff_overall_column.to_frame()\n",
    "        if count == 0:\n",
    "            combined_df = pd.concat([empty_df, country_diff_overall_column_df], axis=1)\n",
    "            count = 1\n",
    "        elif count > 0:\n",
    "            combined_df = pd.concat([combined_df, country_diff_overall_column_df], axis=1)\n",
    "        #print (country_diff_overall_column_df)\n",
    "        combined_df = combined_df.rename(columns={'diff_overall': i})\n",
    "    return (combined_df)\n",
    "\n",
    "several_countries(unique_locations_Europe, empty_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_of_interest = \"Finland\"\n",
    "filtered = data_without_OWID.loc[df['location'] == country_of_interest]\n",
    "filtered = filtered.reset_index(drop=True)\n",
    "vaccinations = filtered[\"total_vaccinations\"]\n",
    "vaccinations.head(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finland and other countries only reporting once per week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Handling of missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure\n",
    "1. Decision to delete columns 63:67\n",
    "2. Decision to delete entire countries\n",
    "3. Decision over keeping or deleting/imputing columns 0:16 (Nico)\n",
    "4. Decision over keeping or deleting/imputing columns 16:32 (Leon)\n",
    "5. Decision over keeping or deleting/imputing columns 32:48 (Florin)\n",
    "6. Decision over keeping or deleting/imputing columns 48:63 (Sevi)\n",
    "7. Actual imputations and deletions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Decision to delete columns 63:67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_percentages = df.iloc[:,63:].isnull().sum() / df.shape[0] *100\n",
    "print(missing_percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than 95% of entries are missing. These four columns will be deleted entirely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decision to delete entire countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 What countries miss how many rows entirely?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What countries are missing entire columns? Helps us determine if we should maybe just remove the countries instead of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data = []\n",
    "for column in df.columns:\n",
    "    grouped_by_iso = df.groupby(\"iso_code\")\n",
    "    nan_counts = grouped_by_iso[column].apply(lambda x: x.isnull().all())\n",
    "    result_data.extend([{'ISO_Code': iso_code, 'Column': column, 'All_NaN': all_nan} for iso_code, all_nan in zip(nan_counts.index, nan_counts.values)])\n",
    "\n",
    "result_df = pd.DataFrame(result_data)\n",
    "most_true_iso_code = result_df.loc[result_df['All_NaN']].groupby('ISO_Code').size().sort_values(ascending=False)\n",
    "\n",
    "print(\"ISO code with the most occurrences of completely missing entire columns in their respective row:\", most_true_iso_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OWID seems more and more useless. Maybe also delete other countries entirely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to find out where to make the cutoff point for the countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20_missing_complete_loc= most_true_iso_code.head(20)\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.bar(top20_missing_complete_loc.index, top20_missing_complete_loc)\n",
    "plt.xlabel(\"iso codes\")\n",
    "plt.ylabel(\"Number of completely missing columns\")\n",
    "plt.tight_layout()\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This topic of row/country deletion has to be visited again after deleting columns that can be deleted for sure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Decision over keeping or deleting/imputing columns 0:16 (Nico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Missing data percentages (Nico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_nico = data.columns[0:16]\n",
    "missing_data_nico = df[columns_nico].isnull().sum() \n",
    "missing_data_nico_percentage = missing_data_nico / df.shape[0] * 100\n",
    "print(missing_data_nico_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the first 4 columns are all (except continent) complete. This makes sense since they contain information about the country and date and did not have to be measured.\n",
    "In general the amount of missing values is very low. For this reason and since cases and deaths are our Label, we cannot delete a column from this part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Missing data per country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we will look if there are certain countries missing a huge amount of data. If that were the case we could delete the entries of those couuntries. To get an overview I created a plot for each continent containing all the countries and their respective missing entries in the first 16 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pd.DataFrame(filtering_missing_country('Finland'))['# of missing entries'][0:16]) \n",
    "#print(locations_per_continent('Europe'))\n",
    "\n",
    "def missingdata_country_nico(continent):\n",
    "    missing_data_countrylist_nico = []\n",
    "    \n",
    "    for country in locations_per_continent(continent):\n",
    "        missing_data_country_nico = pd.DataFrame(filtering_missing_country(country))\n",
    "        sum_missing_entries_nico = missing_data_country_nico['# of missing entries'][0:16].sum()\n",
    "        missing_data_countrylist_nico.append({'Country': country, 'Missing Entries': sum_missing_entries_nico})\n",
    "        #print(f\"{country}: {sum_missing_entries_nico}\")\n",
    "    missing_data_country_df_nico = pd.DataFrame(missing_data_countrylist_nico)\n",
    "    \n",
    "    # Create countplot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Country', y='Missing Entries', data=missing_data_country_df_nico)\n",
    "    plt.title('Missing Entries per Country in ' + continent)\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x labels for better readability\n",
    "    plt.xlabel('Country')\n",
    "    plt.ylabel('Number of Missing Entries')\n",
    "    plt.tight_layout()\n",
    "    safepoint = \"../output/Missing_Data_Analysis/cols0-15/missing_data-\" + continent +\".png\"\n",
    "    plt.savefig(safepoint)\n",
    "\n",
    "\n",
    "for continent in continents_of_interest:\n",
    "    missingdata_country_nico(continent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clearly visible that there are a few outliers, which have a lot of missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part of the code I wanted to check if the 'per million' numbers are deducted from the total numbers. This was done with the total cases and toatal cases per million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in locations_per_continent('Europe'):\n",
    "    country_entries_totalcases = df[df[columns_nico[2]] == country].iloc[:, 4]\n",
    "    country_entries_totalcasesmillion = df[df[columns_nico[2]] == country].iloc[:, 10]\n",
    "    country_entries_missing_totalcases = country_entries_totalcases.isnull().sum()\n",
    "    country_entries_missing_totalcasesmillion = country_entries_totalcasesmillion.isnull().sum()\n",
    "    print(country, country_entries_missing_totalcases, country_entries_missing_totalcasesmillion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results make sense and show that the 'per million' entries are probably deducted from the total numbers. This means the amount of missing entries per country is the same in those two columns. This means we only have to concentrate on the column total cases, to get a feedback on how many entries in general are missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to find out how many percentage of the data are missing in each country in the columns total cases and total deaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingdata_cases_nico(continent):\n",
    "    missing_data_caseslist_nico = []\n",
    "    \n",
    "    for country in locations_per_continent(continent):\n",
    "        country_cases_nico = df[df[columns_nico[2]] == country].iloc[:, 4]\n",
    "        country_casesmissing_nico = country_cases_nico.isnull().sum()\n",
    "        country_casesmissing_nico_percentage = (country_casesmissing_nico / len(country_cases_nico)) * 100\n",
    "        if country_casesmissing_nico_percentage == 100:\n",
    "            print('Hier fehlen 100% von den Daten: ' + country)\n",
    "        missing_data_caseslist_nico.append({'Country': country, 'Missing Entries': country_casesmissing_nico_percentage})\n",
    "        #print(f\"{country}: {sum_missing_entries_nico}\")\n",
    "    country_missingcases_df_nico = pd.DataFrame(missing_data_caseslist_nico)\n",
    "    #print(country_missingcases_df_nico)\n",
    "    \n",
    "    # Create countplot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Country', y='Missing Entries', data=country_missingcases_df_nico)\n",
    "    plt.title('Percentage of Missing Entries in Total Cases per Country in ' + continent)\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x labels for better readability\n",
    "    plt.xlabel('Country')\n",
    "    plt.ylabel('Percentage of Missing Entries in total cases')\n",
    "    plt.tight_layout()\n",
    "    safepoint = \"../output/Missing_Data_Analysis/cols0-15/missing_totalcases-\" + continent +\".png\"\n",
    "    plt.savefig(safepoint)\n",
    "    \n",
    "\n",
    "for continent in continents_of_interest:\n",
    "    missingdata_cases_nico(continent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all of these countries all entries in total cases are missing: Puerto Rico, Sint Maarten (Dutch part), United States Virgin Islands, Turkmenistan, Guernsey, Jersey, Western Sahara, Guam, Niue, Northern Mariana Islands, Pitcairn, Tokelau. I recommend to delete these countries.\n",
    "Now we will have a look at the column total deaths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingdata_deaths_nico(continent):\n",
    "    missing_data_deathslist_nico = []\n",
    "    \n",
    "    for country in locations_per_continent(continent):\n",
    "        country_deaths_nico = df[df[columns_nico[2]] == country].iloc[:, 7]\n",
    "        country_deathsmissing_nico = country_deaths_nico.isnull().sum()\n",
    "        country_deathsmissing_nico_percentage = (country_deathsmissing_nico / len(country_deaths_nico)) * 100\n",
    "        if country_deathsmissing_nico_percentage == 100:\n",
    "            print('Hier fehlen 100% von den Daten: ' + country)\n",
    "        missing_data_deathslist_nico.append({'Country': country, 'Missing Entries': country_deathsmissing_nico_percentage})\n",
    "        #print(f\"{country}: {sum_missing_entries_nico}\")\n",
    "    country_missingdeaths_df_nico = pd.DataFrame(missing_data_deathslist_nico)\n",
    "    #print(country_missingdeaths_df_nico)\n",
    "\n",
    "\n",
    "    # Create countplot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Country', y='Missing Entries', data=country_missingdeaths_df_nico)\n",
    "    plt.title('Percentage of Missing Entries in Total Deaths per Country in ' + continent)\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x labels for better readability\n",
    "    plt.xlabel('Country')\n",
    "    plt.ylabel('Percentage of Missing Entries in total deaths')\n",
    "    plt.tight_layout()\n",
    "    safepoint = \"../output/Missing_Data_Analysis/cols0-15/missing_totaldeaths-\" + continent +\".png\"\n",
    "    plt.savefig(safepoint)\n",
    "    \n",
    "\n",
    "for continent in continents_of_interest:\n",
    "    missingdata_deaths_nico(continent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countries in which all the entries in the column total deaths are missing are: Puerto Rico,Sint Maarten (Dutch part),United States Virgin Islands, Turkmenistan, Falkland Islands, Guernsey, Jersey, Vatican, Saint Helena, Western Sahara, Guam, Niue, Northern Mariana Islands, Pitcairn, Tokelau, Tuvalu. \n",
    "\n",
    "These contain also all the countries which were missing all entries in the column total cases. My recommendation is to delete these countries from the dataset.\n",
    "\n",
    "Finally we will have a look at the regions in the OWID part of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingdata_cases_nico_OWID():\n",
    "    missing_data_caseslist_OWID_nico = []\n",
    "    \n",
    "    for location in loc_OWID:\n",
    "        country_casesOWID_nico = df[df[columns_nico[2]] == location].iloc[:, 4]\n",
    "        country_casesmissingOWID_nico = country_casesOWID_nico.isnull().sum()\n",
    "        country_casesmissingOWID_nico_percentage = (country_casesmissingOWID_nico / len(country_casesOWID_nico)) * 100\n",
    "        if country_casesmissingOWID_nico_percentage == 100:\n",
    "            print('Hier fehlen 100% von den Daten: ' + location)\n",
    "        missing_data_caseslist_OWID_nico.append({'Country': location, 'Missing Entries': country_casesmissingOWID_nico_percentage})\n",
    "        #print(f\"{country}: {sum_missing_entries_nico}\")\n",
    "    country_missingcasesOWID_df_nico = pd.DataFrame(missing_data_caseslist_OWID_nico)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Country', y='Missing Entries', data=country_missingcasesOWID_df_nico)\n",
    "    plt.title('Percentage of Missing Entries in Total Cases per Location in OWID')\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x labels for better readability\n",
    "    plt.xlabel('Location')\n",
    "    plt.ylabel('Percentage of Missing Entries in total cases')\n",
    "    plt.tight_layout()\n",
    "    safepoint = \"../output/Missing_Data_Analysis/cols0-15/missing_totalcases-OWID.png\"\n",
    "    plt.savefig(safepoint)\n",
    "    \n",
    "\n",
    "missingdata_cases_nico_OWID()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingdata_deaths_nico_OWID():\n",
    "    missing_data_deathslist_OWID_nico = []\n",
    "    \n",
    "    for location in loc_OWID:\n",
    "        country_deathsOWID_nico = df[df[columns_nico[2]] == location].iloc[:, 7]\n",
    "        country_deathsmissingOWID_nico = country_deathsOWID_nico.isnull().sum()\n",
    "        country_deathsmissingOWID_nico_percentage = (country_deathsmissingOWID_nico / len(country_deathsOWID_nico)) * 100\n",
    "        if country_deathsmissingOWID_nico_percentage == 100:\n",
    "            print('Hier fehlen 100% von den Daten: ' + location)\n",
    "        missing_data_deathslist_OWID_nico.append({'Country': location, 'Missing Entries': country_deathsmissingOWID_nico_percentage})\n",
    "        #print(f\"{country}: {sum_missing_entries_nico}\")\n",
    "    country_missingdeathsOWID_df_nico = pd.DataFrame(missing_data_deathslist_OWID_nico)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Country', y='Missing Entries', data=country_missingdeathsOWID_df_nico)\n",
    "    plt.title('Percentage of Missing Entries in Total Deaths per Location in OWID')\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x labels for better readability\n",
    "    plt.xlabel('Location')\n",
    "    plt.ylabel('Percentage of Missing Entries in total deaths')\n",
    "    plt.tight_layout()\n",
    "    safepoint = \"../output/Missing_Data_Analysis/cols0-15/missing_totaldeaths-OWID.png\"\n",
    "    plt.savefig(safepoint)\n",
    "    \n",
    "\n",
    "missingdata_deaths_nico_OWID()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the regions England, Northern Ireland, Northern Cyprus, Scotland and Wales are missing all datapoints. I recommend to delete these regions too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Decision over keeping or deleting/imputing columns 16:32 (Leon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Missing data percentages (Leon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_leon = data.columns[16:32]\n",
    "missing_percentages_leon = df[columns_leon].isnull().sum() / df.shape[0] *100\n",
    "print(missing_percentages_leon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance it only makes sense to keep reproduction rate and maybe the ones with percentages of missing data below 75%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Visualization of variables of Interest (Leon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing all my variables for three countries to determine usefulness and worth of imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in columns_leon:    \n",
    "    belgium_data = df.groupby(\"iso_code\").get_group(\"BEL\")\n",
    "    belgium = belgium_data[column].fillna(0)\n",
    "\n",
    "    brazil_data = df.groupby(\"iso_code\").get_group(\"BRA\")\n",
    "    brazil = brazil_data[column].fillna(0)\n",
    "\n",
    "    mexico_data = df.groupby(\"iso_code\").get_group(\"MEX\")\n",
    "    mexico = mexico_data[column].fillna(0)\n",
    "\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(8, 6))\n",
    "\n",
    "    # Plot data on each subplot\n",
    "    axs[0].plot(belgium_data[\"date\"], belgium)\n",
    "    axs[0].set_xlabel(\"date\")\n",
    "    axs[0].set_ylabel(column)\n",
    "    axs[0].set_title(\"Belgium\")\n",
    "\n",
    "    axs[1].plot(brazil_data[\"date\"], brazil)\n",
    "    axs[1].set_xlabel(\"date\")\n",
    "    axs[1].set_ylabel(column)\n",
    "    axs[1].set_title(\"Brazil\")\n",
    "\n",
    "    axs[2].plot(mexico_data[\"date\"], mexico)\n",
    "    axs[2].set_xlabel(\"date\")\n",
    "    axs[2].set_ylabel(column)\n",
    "    axs[2].set_title(\"Mexico\")\n",
    "\n",
    "    # Adjust layout to prevent overlapping\n",
    "    plt.tight_layout()\n",
    "    safepoint = \"../output/Missing_Data_Analysis/cols16-32/missing_data-\" + column +\".png\"\n",
    "    plt.savefig(safepoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of them seem to be imputable. We will keep all variables for the cases since we will target number of cases in our prediction. The ones that were missing 75% plus of their values are also the ones that look like they will not be imputable by any means. Brazil also seems to be lacking positive rate completely (which was under 75% missing values but not in the cases group). Positive rate and reproduction rate are the only ones left to determine whether we will keep them or delete them (The ones with 75% plus missing will most likely be deleted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 How many countries are entirely missing my variables of interest? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_countries = list(df.groupby(\"iso_code\").groups.keys())\n",
    "for column in columns_leon:\n",
    "    grouped_by_iso = df.groupby(\"iso_code\")\n",
    "    nan_counts = grouped_by_iso[column].apply(lambda x: x.isnull().all()).sum()\n",
    "    print(\"Percentage of countries that completely miss all values of\", column,\":\", nan_counts/len(all_countries)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This again shows that the variables that miss more than 75% of their values are also very much the ones where the highest percentage of countries do not have any values of them stored at all. (90% means that 90 percent of all countries do not have a single value but Nan for this column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to keep all variables containing the tests, we would have to delete rows that contain only Nan for those columns. For what countries is that the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_variables = [var for var in columns_leon if 'test' in var]\n",
    "empty_countries_test = []\n",
    "for column in test_variables:  \n",
    "    test_nan_countries = grouped_by_iso[column].apply(lambda x: x.isnull().all())\n",
    "    test_nan_countries_true = test_nan_countries[test_nan_countries == True]\n",
    "    empty_countries_test.append(test_nan_countries_true.index)\n",
    "empty_countries_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what country is missing how many test variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_dict = {}\n",
    "for countries in empty_countries_test:\n",
    "    for country in countries:\n",
    "        existing_country = counter_dict.get(country)\n",
    "        if existing_country is not None:\n",
    "            counter_dict[country] += 1\n",
    "        else:\n",
    "            counter_dict[country] = 1\n",
    "sorted_counter_dict = dict(sorted(counter_dict.items(), key=lambda item:item[1]))\n",
    "print(sorted_counter_dict)\n",
    "sorted_counter_dict.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countries missing all 6 columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_without_tests = [key for key, value in counter_dict.items() if value == 6]\n",
    "countries_without_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to keep all test variables, we will have to delete all of those countries listed above. We will for sure delete the countries missing all 6 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Decision over keeping or deleting/imputing columns 32:48 (Florin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Functions for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.11 Function to replace missing data with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here is how to replace the mssing values with zero values\n",
    "def replace_missing_data(country):\n",
    "    filtered_country = data_without_OWID.loc[df['location'] == country]\n",
    "    filtered_country = filtered_Italy.reset_index(drop=True)\n",
    "    missing_vals = [\"NA\", \"\", None, np.NaN]\n",
    "    missing_country = filtered_country.isin(missing_vals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.12 Function for missing values excluding missing at the end and at the beginning for a specific country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering_missing_country(country):\n",
    "    filtered_country = data_without_OWID.loc[df['location'] == country]\n",
    "    filtered_country = filtered_country.reset_index(drop=True)\n",
    "    filtered_country_summary = filtered_country.isnull().sum()\n",
    "    first_valid_indices_country = filtered_country.apply(lambda x: x.first_valid_index())\n",
    "    last_valid_indices_country = filtered_country.apply(lambda x: x.last_valid_index())\n",
    "    length = len(filtered_country) - 1\n",
    "\n",
    "    filtered_country_summary_df = filtered_country_summary.to_frame()\n",
    "    first_valid_indices_country_df = first_valid_indices_country.to_frame()\n",
    "    last_valid_indices_country_df = last_valid_indices_country.to_frame()\n",
    "    list_of_dataframes = [filtered_country_summary_df, first_valid_indices_country_df, last_valid_indices_country]\n",
    "\n",
    "    comparison_country = pd.merge(filtered_country_summary_df, first_valid_indices_country_df, left_index=True, right_index=True)\n",
    "    comparison_country = comparison_country.rename(columns={\"0_x\": '# of missing entries', \"0_y\": \"first entry\"})\n",
    "    diff_first = filtered_country_summary_df - first_valid_indices_country_df\n",
    "    last_valid_indices_country_df_diff = length - last_valid_indices_country_df\n",
    "    diff_overall = filtered_country_summary_df - first_valid_indices_country_df - last_valid_indices_country_df_diff\n",
    "    comparison_country = comparison_country.assign(diff_first = diff_first, \\\n",
    "                last_entry= last_valid_indices_country_df, diff_last = last_valid_indices_country_df_diff, \\\n",
    "                diff_overall = diff_overall)\n",
    "    return (comparison_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering_missing_country(\"Germany\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_countr = data_without_OWID.loc[df['location'] == \"Germany\"]\n",
    "columns_to_print = [\"location\", \"tests_per_case\"]\n",
    "data_countr = data_countr[columns_to_print]\n",
    "data_countr.head(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.13 Function total diff for many countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locations_per_continent(continent):\n",
    "    filtered_continent = data_without_OWID.loc[df['continent'] == continent]\n",
    "    unique_values_continent = filtered_continent['location'].unique()\n",
    "    return unique_values_continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_locations_Europe = locations_per_continent(\"Europe\")\n",
    "empty_df = pd.DataFrame()\n",
    "\n",
    "def several_countries_diff_overall_specific_columns(list_of_countries, empty_dataframe):\n",
    "    count = 0\n",
    "    for i in (list_of_countries):\n",
    "        country = filtering_missing_country(i)\n",
    "        #print (country)\n",
    "        country_diff_overall_column = country[\"diff_overall\"]\n",
    "        #print (country_diff_overall_column)\n",
    "        country_diff_overall_column_df = country_diff_overall_column.to_frame()\n",
    "        if count == 0:\n",
    "            combined_df = pd.concat([empty_df, country_diff_overall_column_df], axis=1)\n",
    "            count = 1\n",
    "        elif count > 0:\n",
    "            combined_df = pd.concat([combined_df, country_diff_overall_column_df], axis=1)\n",
    "        #print (country_diff_overall_column_df)\n",
    "        combined_df = combined_df.rename(columns={'diff_overall': i})\n",
    "    return (combined_df)\n",
    "\n",
    "output = several_countries_diff_overall_specific_columns(unique_locations_Europe, empty_df)\n",
    "output.iloc[list(range(2, 3)) + list(range(32, 48)),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshhold = 50\n",
    "unique_locations_world = data_without_OWID['location'].unique()\n",
    "empty_df = pd.DataFrame()\n",
    "dataset = unique_locations_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countries_to_check(column, threshhold_value):\n",
    "    output = several_countries_diff_overall_specific_columns(dataset, empty_df)\n",
    "    output_filter = output.loc[column] #filtering for the specific column that we are interested in\n",
    "    output_filter = output_filter.to_frame()\n",
    "    output_filter = output_filter.dropna(axis = 0)\n",
    "    output_filter = output_filter[output_filter > threshhold_value]\n",
    "    output_filter = output_filter.dropna(axis = 0)\n",
    "    return output_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countries_eligible(column, threshhold_value):\n",
    "    output = several_countries_diff_overall_specific_columns(dataset, empty_df)\n",
    "    output_filter = output.loc[column] #filtering for the specific column that we are interested in\n",
    "    output_filter = output_filter.to_frame()\n",
    "    output_filter = output_filter.dropna(axis = 0)\n",
    "    output_filter = output_filter[output_filter < threshhold_value]\n",
    "    output_filter = output_filter.dropna(axis = 0)\n",
    "    return output_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_countr = data_without_OWID.loc[df['location'] == \"Germany\"]\n",
    "columns_to_print = [\"date\", \"total_vaccinations\"]\n",
    "data_countr = data_countr[columns_to_print]\n",
    "data_countr.head(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.14 Function to check whether there is some pattern in how often the data is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularity(country, column): #this function is to check how regular the reporting takes place\n",
    "    data_countr = data_without_OWID.loc[df['location'] == country] #filtering the country of interest, including all columns\n",
    "    data_countr = data_countr[column] #+filtering the column of interest for the country of interest\n",
    "    data_countr = data_countr.reset_index(drop=True) #resetting the index to 0\n",
    "    data_countr = data_countr.to_frame() #converting it to a pandas dataframe\n",
    "    first_valid_index = data_countr.apply(lambda x: x.first_valid_index()) #finding the first valid index\n",
    "    first_valid_index = first_valid_index[0] #isolating just the index number\n",
    "    last_valid_index = data_countr.apply(lambda x: x.last_valid_index()) #finding the last valid index\n",
    "    last_valid_index = last_valid_index[0] #isolating the last valid index number\n",
    "    data_countr_filtered = data_countr.iloc[first_valid_index:last_valid_index, :] #filtering the data so we disregard the first values where we have no value and the last values\n",
    "    data_countr_filtered = data_countr_filtered.isna().sum(axis=1) #1 where there is no data and zero where there is data\n",
    "    data_countr_filtered = data_countr_filtered.replace({0: 1, 1: 0})\n",
    "    return data_countr_filtered\n",
    "\n",
    "#regularity(\"Germany\", \"total_vaccinations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.15 Function to transform the dataset of regularity before plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plotting_values = np.array([1,0,1,0,0,0,1,1,1,0,1,0,0,0,0,1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,1,1,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,0])\n",
    "Brazil_data = regularity(\"Brazil\", \"total_vaccinations\")\n",
    "Brazil_data = Brazil_data.values\n",
    "\n",
    "def transform(plotting_values):\n",
    "    count_month = 2\n",
    "    counter = 0\n",
    "    for i in range(len(plotting_values)):\n",
    "        counter += 1\n",
    "        if plotting_values[i] == 1:\n",
    "            plotting_values[i] = count_month\n",
    "        #print (plotting_values[i])\n",
    "        if counter == 30:\n",
    "            counter = 0\n",
    "            count_month += 1\n",
    "            #print (\"printing : \", count_month)\n",
    "    return plotting_values\n",
    "\n",
    "transform(plotting_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.16 Function to plot these patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neue_liste = [\"Armenia\", \"Albania\", \"Bahamas\", \"Brazil\", \"Germany\", \"France\", \"Argentina\", \"Bolivia\"]\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def plotting_regularity(neue_liste): #this function is to plot the regularity graph\n",
    "    num_plots = len(neue_liste)\n",
    "    num_cols = 3  # Number of rows\n",
    "    num_rows = (num_plots // num_cols) + 1\n",
    "    fig_width = 6 * num_cols\n",
    "    fig_height = 2 * num_plots\n",
    "    j = 0\n",
    "    k = 0\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(fig_width, fig_height)) #creating as many subplots as the list is long\n",
    "    for i, dataset in enumerate(neue_liste): #iterating over the list\n",
    "        #print (\"printing i: \", i, \"and j and k\", j, k )\n",
    "        filtered_data_new = regularity(neue_liste[i], \"total_vaccinations\") #calling the regularity function to get the data\n",
    "        filtered_data_new = filtered_data_new.values #converting the datatype\n",
    "        filtered_data_new = transform(filtered_data_new)\n",
    "        filtered_data_new = filtered_data_new[filtered_data_new != 0]\n",
    "        axs[j,k].hist(filtered_data_new, bins=50, edgecolor='black')  # Plot histogram\n",
    "        axs[j,k].set_title(neue_liste[i])  # Add title\n",
    "        axs[j,k].set_xlabel('Index')  # Add x-axis label\n",
    "        axs[j,k].set_ylabel('Number of entries')  # Add y-axis label\n",
    "        axs[j,k].grid(True)\n",
    "        if k == num_cols - 1:\n",
    "            k = 0\n",
    "            j += 1\n",
    "        else: \n",
    "            k += 1\n",
    "    plt.tight_layout()  # Adjust layout\n",
    "    #display(HTML(\"<style> .output { overflow-y: auto; max-height: 200px; } </style>\"))\n",
    "    plt.show()  # Show the plot  \n",
    "\n",
    "plotting_regularity(neue_liste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Individual column analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.1 Analysis \"stringency index\" (complete set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analysis \"stringency_index\"\n",
    "countries_eligible_stringency_index = countries_eligible(\"stringency_index\", 50)\n",
    "countries_to_check_stringency_index = countries_to_check(\"stringency_index\", 50)\n",
    "index_list_to_check_stringency_index = countries_to_check_stringency_index.index.tolist()\n",
    "index_list_eligible_stringency_index = countries_eligible_stringency_index.index.tolist()\n",
    "#index_list_to_check_stringency_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stringency_index = data[\"stringency_index\"]\n",
    "data_stringency_index.head(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_regularity(index_list_eligible_stringency_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.2 Analysis \"new_people_vaccinated_smoothed_per_hundred\" (complete set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analysis \"new_people_vaccinated_smoothed_per_hundred\"\n",
    "countries_eligible_new_people_vaccinated_smoothed_per_hundred = countries_eligible(\"new_people_vaccinated_smoothed_per_hundred\", 50)\n",
    "countries_to_check_new_people_vaccinated_smoothed_per_hundred = countries_to_check(\"new_people_vaccinated_smoothed_per_hundred\", 50)\n",
    "index_list_to_check_new_people_vaccinated_smoothed_per_hundred = countries_to_check_new_people_vaccinated_smoothed_per_hundred.index.tolist()\n",
    "index_list_eligible_new_people_vaccinated_smoothed_per_hundred = countries_eligible_new_people_vaccinated_smoothed_per_hundred.index.tolist()\n",
    "#index_list_to_check_new_people_vaccinated_smoothed_per_hundred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.3 Analysis new_people_vaccinated_smoothed (complete set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analysis \"new_people_vaccinated_smoothed\"\n",
    "countries_eligible_new_people_vaccinated_smoothed = countries_eligible(\"new_people_vaccinated_smoothed\", 50)\n",
    "countries_to_check_new_people_vaccinated_smoothed = countries_to_check(\"new_people_vaccinated_smoothed\", 50)\n",
    "index_list_to_check_new_people_vaccinated_smoothed = countries_to_check_new_people_vaccinated_smoothed.index.tolist()\n",
    "index_list_eligible_new_people_vaccinated_smoothed = countries_eligible_new_people_vaccinated_smoothed.index.tolist()\n",
    "#index_list_to_check_new_people_vaccinated_smoothed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.4 Analysis new_vaccinations_smoothed_per_million (complete set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analysis \"new_vaccinations_smoothed_per_million\"\n",
    "countries_eligible_new_vaccinations_smoothed_per_million = countries_eligible(\"new_vaccinations_smoothed_per_million\", 50)\n",
    "countries_to_check_new_vaccinations_smoothed_per_million = countries_to_check(\"new_vaccinations_smoothed_per_million\", 50)\n",
    "index_list_to_check_new_vaccinations_smoothed_per_million = countries_to_check_new_vaccinations_smoothed_per_million.index.tolist()\n",
    "index_list_eligible_new_vaccinations_smoothed_per_million = countries_eligible_new_vaccinations_smoothed_per_million.index.tolist()\n",
    "#index_list_to_check_new_vaccinations_smoothed_per_million"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.5 Analysis \"total_boosters_per_hundred\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: dropping Algeria, Niue, Tokelau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analysis \"total_boosters_per_hundred\"\n",
    "countries_eligible_total_boosters_per_hundred = countries_eligible(\"total_boosters_per_hundred\", 50)\n",
    "countries_to_check_total_boosters_per_hundred = countries_to_check(\"total_boosters_per_hundred\", 50)\n",
    "index_list_to_check_total_boosters_per_hundred = countries_to_check_total_boosters_per_hundred.index.tolist()\n",
    "index_list_eligible_total_boosters_per_hundred = countries_eligible_total_boosters_per_hundred.index.tolist()\n",
    "#plotting_regularity(index_list_to_check_total_boosters_per_hundred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.6 Analysis people_fully_vaccinated_per_hundred\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: Algeria, Nauru, Bonnaire, Pitcern, Niue, Tokelau, Turkmenistan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analysis \"people_fully_vaccinated_per_hundred\"\n",
    "countries_eligible_people_fully_vaccinated_per_hundred = countries_eligible(\"people_fully_vaccinated_per_hundred\", 50)\n",
    "countries_to_check_people_fully_vaccinated_per_hundred = countries_to_check(\"people_fully_vaccinated_per_hundred\", 50)\n",
    "index_list_to_check_people_fully_vaccinated_per_hundred = countries_to_check_people_fully_vaccinated_per_hundred.index.tolist()\n",
    "index_list_eligible_people_fully_vaccinated_per_hundred = countries_eligible_people_fully_vaccinated_per_hundred.index.tolist()\n",
    "#plotting_regularity(index_list_to_check_people_fully_vaccinated_per_hundred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.7 Analysis \"people_vaccinated_per_hundred\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: dropping Algeria, Bonnaire, Falkland islands, Pitcern, Niue, St Helena, Tokelau, Turkmenistan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analysis \"people_vaccinated_per_hundred\"\n",
    "countries_eligible_people_vaccinated_per_hundred = countries_eligible(\"people_vaccinated_per_hundred\", 50)\n",
    "countries_to_check_people_vaccinated_per_hundred = countries_to_check(\"people_vaccinated_per_hundred\", 50)\n",
    "index_list_to_check_people_vaccinated_per_hundred = countries_to_check_people_vaccinated_per_hundred.index.tolist()\n",
    "index_list_eligible_people_vaccinated_per_hundred = countries_eligible_people_vaccinated_per_hundred.index.tolist()\n",
    "#plotting_regularity(index_list_to_check_people_vaccinated_per_hundred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.8 Analysis \"total_vaccinations_per_hundred\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: dropping Sint, Falkland islands, Pitcern, Niue, St Helena, Turkmenistan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analysis \"total_vaccinations_per_hundred\"\n",
    "countries_eligible_total_vaccinations_per_hundred = countries_eligible(\"total_vaccinations_per_hundred\", 50)\n",
    "countries_to_check_total_vaccinations_per_hundred = countries_to_check(\"total_vaccinations_per_hundred\", 50)\n",
    "index_list_to_check_total_vaccinations_per_hundred = countries_to_check_total_vaccinations_per_hundred.index.tolist()\n",
    "index_list_eligible_total_vaccinations_per_hundred = countries_eligible_total_vaccinations_per_hundred.index.tolist()\n",
    "#plotting_regularity(index_list_to_check_total_vaccinations_per_hundred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.9 Analysis \"new_vaccinations_smoothed\" (complete set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analysis \"new_vaccinations_smoothed\"\n",
    "countries_eligible_new_vaccinations_smoothed = countries_eligible(\"new_vaccinations_smoothed\", 50)\n",
    "countries_to_check_new_vaccinations_smoothed = countries_to_check(\"new_vaccinations_smoothed\", 50)\n",
    "index_list_to_check_new_vaccinations_smoothed = countries_to_check_new_vaccinations_smoothed.index.tolist()\n",
    "index_list_eligible_new_vaccinations_smoothed = countries_eligible_new_vaccinations_smoothed.index.tolist()\n",
    "#index_list_to_check_new_vaccinations_smoothed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.10 Analysis \"new_vaccinations\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: no countries to be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_eligible_new_vaccinations = countries_eligible(\"new_vaccinations\", 50)\n",
    "countries_to_check_new_vaccinations = countries_to_check(\"new_vaccinations\", 50)\n",
    "index_list_to_check_new_vaccinations = countries_to_check_new_vaccinations.index.tolist()\n",
    "index_list_eligible_new_vaccinations = countries_eligible_new_vaccinations.index.tolist()\n",
    "#plotting_regularity(index_list_to_check_new_vaccinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.11 Analysis \"total boosters\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: dropping Niue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analysis \"total_boosters\"\n",
    "countries_eligible_total_boosters = countries_eligible(\"total_boosters\", 50)\n",
    "countries_to_check_total_boosters = countries_to_check(\"total_boosters\", 50)\n",
    "index_list_to_check_total_boosters = countries_to_check_total_boosters.index.tolist()\n",
    "index_list_eligible_total_boosters = countries_eligible_total_boosters.index.tolist()\n",
    "#plotting_regularity(index_list_eligible_total_boosters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_boosters = data[[\"location\", \"total_boosters\"]]\n",
    "data_boosters.head(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.12 Analysis \"people_fully_vaccinated\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: dropping Algeria, Bonnaire, Niue, Pitcern, Turkmenistan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Analysis \"people_fully_vaccinated\"\n",
    "countries_eligible_people_fully_vaccinated = countries_eligible(\"people_fully_vaccinated\", 50)\n",
    "countries_to_check_people_fully_vaccinated = countries_to_check(\"people_fully_vaccinated\", 50)\n",
    "index_list_to_check_people_fully_vaccinated = countries_to_check_people_fully_vaccinated.index.tolist()\n",
    "index_list_eligible_people_fully_vaccinated = countries_eligible_people_fully_vaccinated.index.tolist()\n",
    "#plotting_regularity(index_list_to_check_people_fully_vaccinated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.13 Analysis \"people_vaccinated\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: dropping Sint, Falkland islands, Pitcern, Niue, St Helena, Tokelau, Turkmenistan, Tuvalu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Analysis \"people_vaccinated\"\n",
    "countries_eligible_people_vaccinated = countries_eligible(\"people_vaccinated\", 50)\n",
    "countries_to_check_people_vaccinated = countries_to_check(\"people_vaccinated\", 50)\n",
    "index_list_to_check_people_vaccinated = countries_to_check_people_vaccinated.index.tolist()\n",
    "index_list_eligible_people_vaccinated = countries_eligible_people_vaccinated.index.tolist()\n",
    "#plotting_regularity(index_list_to_check_people_vaccinated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.14 Analysis \"total_vaccinations\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: dropping Bonnaire Sint, Falkland islands, Pitcern, Niue, St Helena, Tokelau, Turkmenistan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_eligible_total_vaccinations = countries_eligible(\"total_vaccinations\", 50)\n",
    "countries_to_check_total_vaccinations = countries_to_check(\"total_vaccinations\", 50)\n",
    "index_list_to_check_total_vaccinations = countries_to_check_total_vaccinations.index.tolist()\n",
    "index_list_eligible_total_vaccinations = countries_eligible_total_vaccinations.index.tolist()\n",
    "#plotting_regularity(index_list_to_check_total_vaccinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.15 Analysis \"tests_units\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: dropping this entire column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#useless column as this is only stating \"tests performed\" or \"NaN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.16 Analysis \"tests per case\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: dropping Cameroon, Curacao and Suriname from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_eligible_tests_per_case = countries_eligible(\"tests_per_case\", 50)\n",
    "countries_to_check_tests_per_case = countries_to_check(\"tests_per_case\", 50)\n",
    "#countries missing completely at random but could be included: Bhutan, Botswana, El Salvador, Libya, Nepal, Palestine, United States Virgin Islands\n",
    "#too many values generally missing: Cameroon, Curacao, Suriname\n",
    "#weekly reporting: Germany\n",
    "#monthly reporting: Hong Kong\n",
    "#repeating values: Ireland\n",
    "countries_to_check_tests_per_case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from the analysis above:\n",
    "1. Vatican needs to be removed\n",
    "\n",
    "countries with complete information in all categories: [Austria, Belgium, Czechia, Denmark, Estonia, France, Italy, Latvia, Norway, Switzerland, Ukraine, United Kingdom, \t]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EU_tests_per_case_every_value = ['Albania', 'Andorra', 'Austria', 'Belarus', 'Belgium',\n",
    "       'Bosnia and Herzegovina', 'Bulgaria', 'Croatia', 'Cyprus',\n",
    "       'Czechia', 'Denmark', 'Estonia', 'Faeroe Islands', 'Finland',\n",
    "       'France', 'Germany', 'Gibraltar', 'Greece', 'Guernsey', 'Hungary',\n",
    "       'Iceland', 'Ireland', 'Isle of Man', 'Italy', 'Jersey', 'Latvia',\n",
    "       'Liechtenstein', 'Lithuania', 'Luxembourg', 'Malta', 'Moldova',\n",
    "       'Monaco', 'Montenegro', 'Netherlands', 'North Macedonia', 'Norway',\n",
    "       'Poland', 'Portugal', 'Romania', 'Russia', 'San Marino', 'Serbia',\n",
    "       'Slovakia', 'Slovenia', 'Spain', 'Sweden', 'Switzerland',\n",
    "       'Ukraine', 'United Kingdom', 'Vatican']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "total_vaccinations_countries with almost no missing data = []\n",
    "total_vaccinations_ountries_weekly_reporting = []\n",
    "total_vaccinations_countries_random_missing_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_of_interest = \"Finland\"\n",
    "filtered = data_without_OWID.loc[df[\"location\"] == country_of_interest]\n",
    "filtered = filtered.reset_index(drop=True)\n",
    "vaccinations = filtered[\"total_vaccinations\"]\n",
    "vaccinations.head(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_name = \"Finland\"\n",
    "data_with_all_rows = filtering_missing_country(country_name)\n",
    "data_with_specific_rows = data_with_all_rows.iloc[list(range(2, 3)) + list(range(32, 48)),:]\n",
    "data_with_specific_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Decision over keeping or deleting/imputing columns 48:63 (Sevi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Rough overview over the percentages of missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_sevi = data.columns[48:63]\n",
    "missing_percentages_sevi = df[columns_sevi].isnull().sum() / df.shape[0] *100\n",
    "print(missing_percentages_sevi.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All  <75% that was previously defined -> no complete deletions\n",
    "\n",
    "7 columns are  <75% and >20% -> further investigation\n",
    "\n",
    "8 columns are <20% -> keep as is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Static Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first tried to analyze the change of the missing values over for a few countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in columns_sevi:    \n",
    "    belgium_data = df.groupby(\"iso_code\").get_group(\"BEL\")\n",
    "    belgium = belgium_data[column].fillna(0)\n",
    "\n",
    "    brazil_data = df.groupby(\"iso_code\").get_group(\"BRA\")\n",
    "    brazil = brazil_data[column].fillna(0)\n",
    "\n",
    "    mexico_data = df.groupby(\"iso_code\").get_group(\"MEX\")\n",
    "    mexico = mexico_data[column].fillna(0)\n",
    "\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(8, 6))\n",
    "\n",
    "    # Plot data on each subplot\n",
    "    axs[0].plot(belgium_data[\"date\"], belgium)\n",
    "    axs[0].set_xlabel(\"date\")\n",
    "    axs[0].set_ylabel(column)\n",
    "    axs[0].set_title(\"Belgium\")\n",
    "\n",
    "    axs[1].plot(brazil_data[\"date\"], brazil)\n",
    "    axs[1].set_xlabel(\"date\")\n",
    "    axs[1].set_ylabel(column)\n",
    "    axs[1].set_title(\"Brazil\")\n",
    "\n",
    "    axs[2].plot(mexico_data[\"date\"], mexico)\n",
    "    axs[2].set_xlabel(\"date\")\n",
    "    axs[2].set_ylabel(column)\n",
    "    axs[2].set_title(\"Mexico\")\n",
    "\n",
    "    # Adjust layout to prevent overlapping\n",
    "    plt.tight_layout()\n",
    "    safepoint = \"../output/Missing_Data_Analysis/cols48-63/missing_data-\" + column +\".png\"\n",
    "    plt.savefig(safepoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the number of values change over time.\n",
    "\n",
    "There is only one value for each country for the whole dataset.\n",
    "\n",
    "This may complicate any impution attempts\n",
    "\n",
    "Here a function to confirm this finding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_constant_values(data, country_col, time_col, variables):\n",
    "    results = {}\n",
    "    for country in data[country_col].unique():\n",
    "        country_data = data[data[country_col] == country]\n",
    "        results[country] = {}\n",
    "        for variable in variables:\n",
    "            unique_values = country_data[variable].dropna().unique()\n",
    "            results[country][variable] = len(unique_values) < 2\n",
    "    return results\n",
    "\n",
    "constant_value_check = check_constant_values(data_without_OWID, 'iso_code', 'date', columns_sevi)\n",
    "\n",
    "for country, variables in constant_value_check.items():\n",
    "    print(f\"Country: {country}\")\n",
    "    for var, is_constant in variables.items():\n",
    "        if is_constant == False:\n",
    "            print(f\"  {var}: varies\")\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Actual imputations and deletions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 First deletions that can already be made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More than 90% of entries are missing for those four variables:\n",
    "data_clean = data.drop([\"excess_mortality_cumulative_absolute\", \"excess_mortality_cumulative\", \"excess_mortality\", \"excess_mortality_cumulative_per_million\"], axis=1, inplace=False)\n",
    "\n",
    "\n",
    "#Country deletions (Needed to clean up Leon data): They miss all 6 test columns, we want to have at least one tests column for our algorithms:\n",
    "data_clean = data_clean[~data_clean['iso_code'].isin(countries_without_tests)]\n",
    "\n",
    "\n",
    "#Leons deletions of columns missing more than 75% of data overall and also over 80% of all countries miss those variables entirely :\n",
    "data_clean = data_clean.drop(['icu_patients', 'icu_patients_per_million', 'hosp_patients', 'hosp_patients_per_million', \n",
    "                              'weekly_icu_admissions', 'weekly_icu_admissions_per_million', 'weekly_hosp_admissions',\n",
    "                              'weekly_hosp_admissions_per_million'],axis=1, inplace=False)\n",
    "\n",
    "#Florin deletions of additional columns :\n",
    "data_clean = data_clean.drop(['tests_units'],axis=1, inplace=False)\n",
    "\n",
    "#Florin country deletions:\n",
    "countries_to_be_deleted_Florin = [\"Algeria\", \"Bonaire Sint Eustatius and Saba\", \"Falkland Islands\", \"Nauru\", \"Niue\", \"Pitcairn\", \"Saint Helena\", \"Tokelau\", \"Turkmenistan\",\"Tuvalu\", \"Vatican\"]\n",
    "data_clean = data_clean[~data_clean['location'].isin(countries_to_be_deleted_Florin)]\n",
    "\n",
    "\n",
    "#insert all your drops and imputations here and save in data_clean!\n",
    "#Von jetzt an nur noch data_clean brauchen\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TO REMOVE ASWELL: ALL OWID (we already have created data frames that dont include OWID)\n",
    "data_clean = data_clean[~data_clean['iso_code'].str.contains('OWID')]\n",
    "\n",
    "\n",
    "print(\"Clean data columns left:\", data_clean.columns.to_list())\n",
    "print(\"Clean data countries left:\", data_clean[\"iso_code\"].unique())\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Data observation after first deletions and further deletions (Leons Columns, countries):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this to see how our data_clean behaves after certain removals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leon_columns_after_first_clean = [\"reproduction_rate\", \"total_tests\", \"new_tests\", \"total_tests_per_thousand\", \n",
    "                                  \"new_tests_per_thousand\", \"new_tests_smoothed\", \n",
    "                                  \"new_tests_smoothed_per_thousand\", \"positive_rate\"]\n",
    "\n",
    "all_countries = list(data_clean.groupby(\"iso_code\").groups.keys())\n",
    "for column in leon_columns_after_first_clean:\n",
    "    grouped_by_iso = data_clean.groupby(\"iso_code\")\n",
    "    nan_counts = grouped_by_iso[column].apply(lambda x: x.isnull().all()).sum()\n",
    "    print(\"Percentage of countries that completely miss all values of\", column,\":\", nan_counts/len(all_countries)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have 19% of the countries that are entirely missing new_tests and new_tests per thousand. Those will need to be removed, after that we still have 4 variables that contain information about tests, which should be enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data_clean.drop([\"new_tests\", \"new_tests_per_thousand\"],axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing what countries are still missing positive_rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_countries_p_rate = []\n",
    "p_rate_loc = grouped_by_iso[\"positive_rate\"].apply(lambda x: x.isnull().all())\n",
    "p_rate_nan_countries_true = p_rate_loc[p_rate_loc == True]\n",
    "empty_countries_p_rate.append(p_rate_nan_countries_true.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting those countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data_clean[~data_clean['iso_code'].isin(empty_countries_p_rate[0].tolist())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing what countries are still missing total_tests_per_thousand: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_countries_ttt = []\n",
    "ttt_loc = grouped_by_iso[\"total_tests_per_thousand\"].apply(lambda x: x.isnull().all())\n",
    "ttt_nan_countries_true = ttt_loc[ttt_loc == True]\n",
    "empty_countries_ttt.append(ttt_nan_countries_true.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting those countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data_clean[~data_clean['iso_code'].isin(empty_countries_ttt[0].tolist())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing what countries are still missing reproduction_rate: (Also heavily affects Nicos data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_countries_r_rate = []\n",
    "r_rate_loc = grouped_by_iso[\"reproduction_rate\"].apply(lambda x: x.isnull().all())\n",
    "r_rate_nan_countries_true = r_rate_loc[r_rate_loc == True]\n",
    "empty_countries_r_rate.append(r_rate_nan_countries_true.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting those countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data_clean[~data_clean['iso_code'].isin(empty_countries_r_rate[0].tolist())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation after further deletions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_countries = list(data_clean.groupby(\"iso_code\").groups.keys())\n",
    "for column in data_clean.columns:\n",
    "    grouped_by_iso = data_clean.groupby(\"iso_code\")\n",
    "    nan_counts = grouped_by_iso[column].apply(lambda x: x.isnull().all()).sum()\n",
    "    print(\"Percentage of countries that completely miss all values of\", column,\":\", nan_counts/len(all_countries)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What countries are left after deletions (Leon):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_left = data_clean.groupby(\"iso_code\").groups.keys()\n",
    "print(countries_left)\n",
    "print(len(countries_left))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My remaining six columns have no more completely empty rows. Now on to the imputation of those columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 Imputations of remaining six columns (Leon):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To impute (Leon): reproduction_rate, total_tests, total_tests_per_thousand, new_tests_smoothed, new_tests_smoothed_per_thousand, positive_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing data percentages for the columns are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leon_columns_after_second_clean =[\"reproduction_rate\", \"total_tests\", \"total_tests_per_thousand\", \n",
    "                                  \"new_tests_smoothed\", \n",
    "                                  \"new_tests_smoothed_per_thousand\", \"positive_rate\"]\n",
    "missing_percentages_leon_after_del = data_clean[leon_columns_after_second_clean].isnull().sum() / data_clean.shape[0] *100\n",
    "print(missing_percentages_leon_after_del)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may seem like a lot for the latter 5 but if we consider that those columns have a lot of those entries missing at the start or the end they are easily imputable. They miss at start and end because countries either have not tracked them from the beginning or have stopped tracking after a certain time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN imputation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data_clean.interpolate(method='linear', subset=leon_columns_after_second_clean)\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "data_clean[leon_columns_after_second_clean] = knn_imputer.fit_transform(data_clean[leon_columns_after_second_clean])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had to first do linear imputations for the end and start parts. There was still missing data after that, thats why there is also the knn imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at how imputation went:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_percentages_leon_after_imp = data_clean[leon_columns_after_second_clean].isnull().sum() / data_clean.shape[0] *100\n",
    "print(missing_percentages_leon_after_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those columns have no more missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in leon_columns_after_second_clean:    \n",
    "    belgium_data = data_clean.groupby(\"iso_code\").get_group(\"BEL\")\n",
    "    belgium = belgium_data[column]\n",
    "\n",
    "    mexico_data = data_clean.groupby(\"iso_code\").get_group(\"MEX\")\n",
    "    mexico = mexico_data[column]\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(8, 5))\n",
    "\n",
    "    # Plot data on each subplot\n",
    "    axs[0].plot(belgium_data[\"date\"], belgium)\n",
    "    axs[0].set_xlabel(\"date\")\n",
    "    axs[0].set_ylabel(column)\n",
    "    axs[0].set_title(\"Belgium\")\n",
    "\n",
    "    axs[1].plot(mexico_data[\"date\"], mexico)\n",
    "    axs[1].set_xlabel(\"date\")\n",
    "    axs[1].set_ylabel(column)\n",
    "    axs[1].set_title(\"Mexico\")\n",
    "\n",
    "    # Adjust layout to prevent overlapping\n",
    "    plt.tight_layout()\n",
    "    safepoint = \"../output/Missing_Data_Analysis/cols16-32/missing_data-\" + column +\"-after_imputation.png\"\n",
    "    plt.savefig(safepoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4 Imputations of columns (Sevi):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-KNN imputation works well for all seven variables, because they are all descriptive variables for a country's socioeconomic state.\n",
    "\n",
    "-It identifies similarities based on multiple factors like health, economy, and demographics, ensuring the imputed values fit well with existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "variables_to_impute = ['handwashing_facilities', 'extreme_poverty', 'male_smokers', \n",
    "                       'female_smokers', 'hospital_beds_per_thousand', 'human_development_index',\n",
    "                       'aged_65_older']\n",
    "data_clean[variables_to_impute] = knn_imputer.fit_transform(data_clean[variables_to_impute])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5 Imputations of columns (Nico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting an overview over the missing data after the first deletions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nico_columns_after_second_clean = data_clean.columns[4:16] #in the first four columns we do not have to impute anything\n",
    "missing_percentages_nico_after_del = data_clean[nico_columns_after_second_clean].isnull().sum() / data_clean.shape[0] *100\n",
    "print(missing_percentages_nico_after_del)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only very little missing data left. All the columns are missing less than 1% which means a simple imputation method like forward/backward fill or mean imputation are sufficient for these columns. In this case we use forward and then backward fill to get rid of all NAN values. This makes sense since most NAN values are at the end or in the beginning of the recording period. The mean value would make less sense because of that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean[nico_columns_after_second_clean] = data_clean[nico_columns_after_second_clean].ffill().bfill() #using forward fill and then backward fill so that no nan are left\n",
    "missing_percentages_nico_after_ffill = data_clean[nico_columns_after_second_clean].isnull().sum() / data_clean.shape[0] *100\n",
    "print(missing_percentages_nico_after_ffill) #checking for missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that there are no more missing values and to check if the imputations make sense, here are two visual representation of the columns from two different countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in nico_columns_after_second_clean:    \n",
    "    finland_data_nico = data_clean.groupby(\"iso_code\").get_group(\"FIN\")\n",
    "    finland_nico = finland_data_nico[column]\n",
    "\n",
    "    mexico_data_nico = data_clean.groupby(\"iso_code\").get_group(\"MEX\")\n",
    "    mexico_nico = mexico_data_nico[column]\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(8, 5))\n",
    "\n",
    "    # Plot data on each subplot\n",
    "    axs[0].plot(finland_data_nico[\"date\"], finland_nico)\n",
    "    axs[0].set_xlabel(\"date\")\n",
    "    axs[0].set_ylabel(column)\n",
    "    axs[0].set_title(\"Finland\")\n",
    "\n",
    "    axs[1].plot(mexico_data_nico[\"date\"], mexico_nico)\n",
    "    axs[1].set_xlabel(\"date\")\n",
    "    axs[1].set_ylabel(column)\n",
    "    axs[1].set_title(\"Mexico\")\n",
    "\n",
    "    # Adjust layout to prevent overlapping\n",
    "    plt.tight_layout()\n",
    "    safepoint = \"../output/Missing_Data_Analysis/cols0-15/missing_data-\" + column +\"-after_imputation.png\"\n",
    "    plt.savefig(safepoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.6 Imupations of columns Forin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN imputations\n",
    "florin_columns_after_cleaning = ['tests_per_case', 'total_vaccinations', 'people_vaccinated', 'people_fully_vaccinated', 'total_boosters', 'new_vaccinations', 'new_vaccinations_smoothed', 'total_vaccinations_per_hundred', 'people_vaccinated_per_hundred', 'people_fully_vaccinated_per_hundred', 'total_boosters_per_hundred', 'new_vaccinations_smoothed_per_million', 'new_people_vaccinated_smoothed', 'new_people_vaccinated_smoothed_per_hundred', 'stringency_index']\n",
    "data_clean = data_clean.interpolate(method='linear', subset=florin_columns_after_cleaning)\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "data_clean[florin_columns_after_cleaning] = knn_imputer.fit_transform(data_clean[florin_columns_after_cleaning])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking how many missing values are left\n",
    "florin_columns_missing_percentages = data_clean[florin_columns_after_cleaning].isnull().sum() / data_clean.shape[0] *100\n",
    "print(florin_columns_missing_percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.7 Clean data overview after imputations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing percentages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_percentages_after_del_imp = data_clean.isnull().sum() / data_clean.shape[0] *100\n",
    "print(missing_percentages_after_del_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Data preprocessing (Handling of missing data excluded) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure\n",
    "1. Looking for outliers\n",
    "2. Distribution of data\n",
    "3. Splitting the data\n",
    "4. Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Looking for outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Looking for outliers using IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "summary_stats = data_clean.describe()\n",
    "Q1 = summary_stats.loc['25%']\n",
    "Q3 = summary_stats.loc['75%']\n",
    "IQR = Q3 - Q1\n",
    "# Define outliers using IQR rule\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "# Align DataFrame columns with summary statistics columns\n",
    "data_clean, lower_bound = data_clean.align(lower_bound, axis=1, join='inner')\n",
    "data_clean, upper_bound = data_clean.align(upper_bound, axis=1, join='inner')\n",
    "outliers = data_clean[(data_clean < lower_bound) | (data_clean > upper_bound)].dropna(axis=1, how='all')\n",
    "if not outliers.empty:\n",
    "    print(\"Outliers:\")\n",
    "    print(outliers)\n",
    "else:\n",
    "    print(\"No outliers found.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Looking for outliers using the Z-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "z_scores = (data_clean - data_clean.mean()) / data_clean.std()\n",
    "outlier_threshold = 3\n",
    "outliers_zscore = (z_scores > outlier_threshold) | (z_scores < -outlier_threshold)\n",
    "\n",
    "outliers_rows = outliers_zscore.any(axis=1)  # Rows containing at least one outlier\n",
    "outliers_cols = outliers_zscore.sum(axis=0) # Amount of outliers per column\n",
    "\n",
    "print(outliers_cols)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Distribution of data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Distribution of numercial data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing whether the data is normally distributed or not:\n",
    "Using the anderson darling test, since the shapiro wilks is not suitable for data n > 5000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data_clean = data_clean.select_dtypes(include='number')\n",
    "distribution_results = {}\n",
    "for column in num_data_clean.columns:\n",
    "    result = sts.anderson(num_data_clean[column].dropna(), dist='norm')  ############################### I had to drop all na for it to work!!! -> maybe imputation needed\n",
    "    test_stat = result.statistic \n",
    "    critical_val = result.critical_values\n",
    "    #print(critical_val)                                \n",
    "    #print(test_stat)                           \n",
    "    if test_stat > critical_val[2]:\n",
    "        result = \"not normal\"\n",
    "    else:\n",
    "        result = \"normal\"    \n",
    "    distribution_results[column] = result\n",
    "\n",
    "\n",
    "not_normal = []\n",
    "for key, value in distribution_results.items():\n",
    "    if value == \"not normal\":\n",
    "        not_normal.append(key)\n",
    "if len(not_normal) == 0:\n",
    "    print(\"All numerical data seems to be normally distributed.\")\n",
    "elif len(not_normal) == len(num_data_clean.columns):\n",
    "    print(\"No variable is normally distributed.\")\n",
    "else:\n",
    "    print(\"All data but\", not_normal, \"seems to be normally distributed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of numercial data visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_per_row = 3\n",
    "num_columns = len(num_data_clean.columns)\n",
    "num_rows = (num_columns - 1) // columns_per_row + 1\n",
    "fig_width = 6 * columns_per_row\n",
    "fig_height = 4 * num_rows\n",
    "fig, axs = plt.subplots(num_rows, 4, figsize=(fig_width, fig_height))\n",
    "axs = axs.flatten()\n",
    "for i, column in enumerate(num_data_clean.columns):\n",
    "    ax = axs[i]\n",
    "    ax.hist(num_data_clean[column], bins=35, color='skyblue', edgecolor='black')\n",
    "    ax.set_title(column)\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../output/Preprocessing/Distributions/distributions_of_variables.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Introduction to the split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to do a cross validation for multiple time frames of our data. Normal cross validation could lead to data leakage because it takes into account future data to predict past data. Time Series Split makes more sense to me. It works as follows:\n",
    "\n",
    "Walk-forward validation is similar to time series cross-validation but focuses on predicting one step ahead using all available past data.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "*First Fold:*\n",
    "\n",
    "- Training: January to March\n",
    "\n",
    "- Validation: April\n",
    "\n",
    "*Second Fold:*\n",
    "\n",
    "- Training: January to April\n",
    "\n",
    "- Validation: May\n",
    "\n",
    "*Third Fold:*\n",
    "\n",
    "- Training: January to May\n",
    "\n",
    "- Validation: June"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to find out how many folds to make:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_day = data_clean[\"date\"].min()\n",
    "last_day = data_clean[\"date\"].max()\n",
    "print(\"Data ranges from\", first_day, \"to\", last_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data and saving the splits in a list. I went for a split of the data so that we have train dates for a length of five months and then one test month. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean_sorted = data_clean.sort_values(by=['date'])\n",
    "all_dates = pd.Series(data_clean_sorted[\"date\"].unique())\n",
    "\n",
    "def custom_time_series_split(all_dates, train_window, test_window):\n",
    "    splits = []\n",
    "\n",
    "    # Define the number of rows to skip for training and testing data\n",
    "    train_skip = train_window\n",
    "    test_skip = test_window\n",
    "\n",
    "    # Iterate over the DataFrame to create train/test splits\n",
    "    for i in range(0, len(all_dates), train_skip + test_skip):\n",
    "        # Check if there are enough rows remaining for both training and testing data\n",
    "        if i + train_skip + test_skip <= len(all_dates):\n",
    "            # Training data\n",
    "            train_data = all_dates.iloc[i:i+train_skip]\n",
    "\n",
    "            # Testing data\n",
    "            test_data = all_dates.iloc[i+train_skip:i+train_skip+test_skip]\n",
    "\n",
    "            # Store the splits\n",
    "            splits.append((train_data, test_data))\n",
    "\n",
    "    return splits\n",
    "\n",
    "# Create the split\n",
    "splits = custom_time_series_split(all_dates, 150, 30)\n",
    "\n",
    "\n",
    "for i, (train_data, test_data) in enumerate(splits):\n",
    "    print(f\"Split {i+1} - Train Data:\")\n",
    "    print(train_data.iloc[0])\n",
    "    print(train_data.iloc[-1])\n",
    "    print(f\"\\nSplit {i+1} - Test Data:\")\n",
    "    print(test_data.iloc[0])\n",
    "    print(test_data.iloc[-1])\n",
    "    print(\"\\n----------------------------------------\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates 6 splits. As one can see, all of 2023 is not included which makes sense, since at the end we had to do most imputations and thus this will be more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the whole data accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_list = []\n",
    "test_data_list = []\n",
    "\n",
    "# Iterate over each split\n",
    "for i, (train_dates, test_dates) in enumerate(splits):\n",
    "    # Filter the original DataFrame based on the train and test dates\n",
    "    train_data = data_clean[data_clean['date'].isin(train_dates)]\n",
    "    test_data = data_clean[data_clean['date'].isin(test_dates)]\n",
    "    \n",
    "    # Append train and test dataframes to respective lists\n",
    "    train_data_list.append(train_data)\n",
    "    test_data_list.append(test_data)\n",
    "\n",
    "# Concatenate all train and test dataframes from different splits\n",
    "for i in range(len(train_data_list)):\n",
    "    # Print the split number\n",
    "    print(f\"Split {i+1}:\")\n",
    "    \n",
    "    # Print the train and test data for the current split\n",
    "    print(\"Train Data:\")\n",
    "    print(train_data_list[i].head())  # You can customize the display as needed\n",
    "    print(\"\\nTest Data:\")\n",
    "    print(test_data_list[i].head())   # You can customize the display as needed\n",
    "    print(\"------------------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This now gives us two lists with data frames in them. train_data_list has the six data frames with the training data and test_data_list contain the six data frames where we can use our models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Finding numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the first 4 columns. Checking for non numeric features in the rest of the columns and deleting those to perform Normalization and Standardization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "#figuring out the numerical features which have to be scaled... Removing the first 4 columns\n",
    "numerical_features = data_clean.columns[4:len(data_clean.columns)]\n",
    "\n",
    "#check for non numeric columns:\n",
    "non_numeric_columns = []\n",
    "for col in numerical_features:\n",
    "    if data_clean[col].dtype not in ['int64', 'float64']:\n",
    "        non_numeric_columns.append(col)\n",
    "\n",
    "# Had to remove the non numeric columns for the Scalers to work... Only column \"tests_units\"... Maybe we can do something else than delete\n",
    "numerical_features = [col for col in numerical_features if col not in non_numeric_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling each split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled train array for Split 1:\n",
      "(array([[-0.10062734, -0.04854369, -0.12681435, ..., -1.13695652,\n",
      "        -1.04313725,  0.77317459],\n",
      "       [-0.10062734, -0.09708738, -0.12681435, ..., -1.13695652,\n",
      "        -1.04313725,  0.77317459],\n",
      "       [-0.10062734, -0.09708738, -0.12681435, ..., -1.13695652,\n",
      "        -1.04313725,  0.77317459],\n",
      "       ...,\n",
      "       [-0.06843673,  0.6407767 , -0.01549548, ..., -1.5       ,\n",
      "        -0.80784314,  0.10957582],\n",
      "       [-0.06412775,  0.06796117,  0.00423142, ..., -1.5       ,\n",
      "        -0.80784314,  0.10957582],\n",
      "       [-0.06412775, -0.09708738,  0.00423142, ..., -1.5       ,\n",
      "        -0.80784314,  0.10957582]]), 0         1.0\n",
      "1         1.0\n",
      "2         1.0\n",
      "3         1.0\n",
      "4         1.0\n",
      "         ... \n",
      "259558    0.0\n",
      "259559    0.0\n",
      "259560    0.0\n",
      "259561    0.0\n",
      "259562    0.0\n",
      "Name: new_deaths, Length: 14026, dtype: float64)\n",
      "scaled test array for Split 1:\n",
      "[[ 3.58076168  7.51456311  6.3154277  ... -1.13695652 -1.04313725\n",
      "   0.77317459]\n",
      " [ 3.74577023  6.22330097  6.22383572 ... -1.13695652 -1.04313725\n",
      "   0.77317459]\n",
      " [ 3.91204613  6.27184466  6.42674657 ... -1.13695652 -1.04313725\n",
      "   0.77317459]\n",
      " ...\n",
      " [ 0.04030163  0.         -0.01831642 ... -1.5        -0.80784314\n",
      "   0.10957582]\n",
      " [ 0.04182244 -0.03883495 -0.00986345 ... -1.5        -0.80784314\n",
      "   0.10957582]\n",
      " [ 0.04182244 -0.09708738 -0.02394845 ... -1.5        -0.80784314\n",
      "   0.10957582]]\n",
      "----------------------------------------\n",
      "scaled train array for Split 2:\n",
      "(array([[ 0.24619084,  0.26036314,  0.28813747, ..., -0.84279476,\n",
      "        -0.94594595,  0.9663081 ],\n",
      "       [ 0.24769831,  0.05481329,  0.24510536, ..., -0.84279476,\n",
      "        -0.94594595,  0.9663081 ],\n",
      "       [ 0.25271905,  0.441247  ,  0.2934915 , ..., -0.84279476,\n",
      "        -0.94594595,  0.9663081 ],\n",
      "       ...,\n",
      "       [-0.02809374,  0.00959233, -0.04580838, ..., -1.13449782,\n",
      "        -0.71428571,  0.17292996],\n",
      "       [-0.02697248,  0.01233299, -0.03747958, ..., -1.13449782,\n",
      "        -0.71428571,  0.17292996],\n",
      "       [-0.02560205,  0.03973964, -0.02280424, ..., -1.13449782,\n",
      "        -0.71428571,  0.17292996]]), 126        4.0\n",
      "127        2.0\n",
      "128       39.0\n",
      "129       33.0\n",
      "130       12.0\n",
      "          ... \n",
      "259738     0.0\n",
      "259739     3.0\n",
      "259740     5.0\n",
      "259741     1.0\n",
      "259742     0.0\n",
      "Name: new_deaths, Length: 23566, dtype: float64)\n",
      "scaled test array for Split 2:\n",
      "[[ 0.42267682  0.17951353  0.19771034 ... -0.84279476 -0.94594595\n",
      "   0.9663081 ]\n",
      " [ 0.42549242  0.19869818  0.16776555 ... -0.84279476 -0.94594595\n",
      "   0.9663081 ]\n",
      " [ 0.4270248   0.05755396  0.14952271 ... -0.84279476 -0.94594595\n",
      "   0.9663081 ]\n",
      " ...\n",
      " [ 0.01361705  0.0424803   0.05255054 ... -1.13449782 -0.71428571\n",
      "   0.17292996]\n",
      " [ 0.01523665  0.06714628  0.05512969 ... -1.13449782 -0.71428571\n",
      "   0.17292996]\n",
      " [ 0.01640774  0.01781432  0.03787659 ... -1.13449782 -0.71428571\n",
      "   0.17292996]]\n",
      "----------------------------------------\n",
      "scaled train array for Split 3:\n",
      "(array([[-0.09360082, -0.06713839, -0.09385997, ..., -0.84235808,\n",
      "        -0.94594595,  0.97118108],\n",
      "       [-0.09337614, -0.12308705, -0.09350341, ..., -0.84235808,\n",
      "        -0.94594595,  0.97118108],\n",
      "       [-0.09295085, -0.07372059, -0.09198989, ..., -0.84235808,\n",
      "        -0.94594595,  0.97118108],\n",
      "       ...,\n",
      "       [-0.12860015, -0.1685042 , -0.19920749, ..., -1.13406114,\n",
      "        -0.71428571,  0.17857388],\n",
      "       [-0.12859213, -0.17640283, -0.19947429, ..., -1.13406114,\n",
      "        -0.71428571,  0.17857388],\n",
      "       [-0.12855468, -0.16916242, -0.19929663, ..., -1.13406114,\n",
      "        -0.71428571,  0.17857388]]), 306       13.0\n",
      "307       10.0\n",
      "308       12.0\n",
      "309       12.0\n",
      "310        7.0\n",
      "          ... \n",
      "259918     2.0\n",
      "259919     1.0\n",
      "259920     0.0\n",
      "259921     0.0\n",
      "259922     0.0\n",
      "Name: new_deaths, Length: 23700, dtype: float64)\n",
      "scaled test array for Split 3:\n",
      "[[-0.05310751  0.23498437  0.06429472 ... -0.84235808 -0.94594595\n",
      "   0.97118108]\n",
      " [-0.05086071  0.3745269   0.11211564 ... -0.84235808 -0.94594595\n",
      "   0.97118108]\n",
      " [-0.04919433  0.23169327  0.12725389 ... -0.84235808 -0.94594595\n",
      "   0.97118108]\n",
      " ...\n",
      " [-0.11919567  0.09544183 -0.02172871 ... -1.13406114 -0.71428571\n",
      "   0.17857388]\n",
      " [-0.11780747  0.16323844  0.00302796 ... -1.13406114 -0.71428571\n",
      "   0.17857388]\n",
      " [-0.11575859  0.32581866  0.05004662 ... -1.13406114 -0.71428571\n",
      "   0.17857388]]\n",
      "----------------------------------------\n",
      "scaled train array for Split 4:\n",
      "(array([[-0.10883946,  0.87108239,  0.97829692, ..., -0.8419214 ,\n",
      "        -0.94594595,  0.97699304],\n",
      "       [-0.10612305,  1.18449111,  0.96560303, ..., -0.8419214 ,\n",
      "        -0.94594595,  0.97699304],\n",
      "       [-0.10410289,  0.84329564,  0.94296791, ..., -0.8419214 ,\n",
      "        -0.94594595,  0.97699304],\n",
      "       ...,\n",
      "       [-0.07711159, -0.12342488, -0.15232357, ..., -1.13362445,\n",
      "        -0.71428571,  0.18438584],\n",
      "       [-0.07711159, -0.14668821, -0.15232357, ..., -1.13362445,\n",
      "        -0.71428571,  0.18438584],\n",
      "       [-0.07708258, -0.13247173, -0.15605176, ..., -1.13362445,\n",
      "        -0.71428571,  0.18438584]]), 486       86.0\n",
      "487       67.0\n",
      "488       75.0\n",
      "489       56.0\n",
      "490       80.0\n",
      "          ... \n",
      "260098     1.0\n",
      "260099     1.0\n",
      "260100     0.0\n",
      "260101     0.0\n",
      "260102     0.0\n",
      "Name: new_deaths, Length: 23791, dtype: float64)\n",
      "scaled test array for Split 4:\n",
      "[[-4.63832596e-02 -1.26009693e-01 -1.45221974e-01 ... -8.41921397e-01\n",
      "  -9.45945946e-01  9.76993041e-01]\n",
      " [-4.63634799e-02 -1.36995153e-01 -1.49039026e-01 ... -8.41921397e-01\n",
      "  -9.45945946e-01  9.76993041e-01]\n",
      " [-4.62263411e-02 -7.94830372e-02 -1.43269020e-01 ... -8.41921397e-01\n",
      "  -9.45945946e-01  9.76993041e-01]\n",
      " ...\n",
      " [-3.30188151e-03 -1.46688207e-01  2.00479384e+00 ... -1.13362445e+00\n",
      "  -7.14285714e-01  1.84385836e-01]\n",
      " [-5.24819824e-04  1.21421648e+00  2.00195295e+00 ... -1.13362445e+00\n",
      "  -7.14285714e-01  1.84385836e-01]\n",
      " [ 3.96647746e-03  2.05428110e+00  2.30429234e+00 ... -1.13362445e+00\n",
      "  -7.14285714e-01  1.84385836e-01]]\n",
      "----------------------------------------\n",
      "scaled train array for Split 5:\n",
      "(array([[-0.13808716, -0.05755773, -0.09312656, ..., -0.8419214 ,\n",
      "        -0.94594595,  0.97561958],\n",
      "       [-0.13807216, -0.05545836, -0.09230736, ..., -0.8419214 ,\n",
      "        -0.94594595,  0.97561958],\n",
      "       [-0.13804996, -0.05125962, -0.09324956, ..., -0.8419214 ,\n",
      "        -0.94594595,  0.97561958],\n",
      "       ...,\n",
      "       [-0.08291855,  0.04671099, -0.05421294, ..., -1.13362445,\n",
      "        -0.71428571,  0.18412663],\n",
      "       [-0.08276312,  0.02641707, -0.04360384, ..., -1.13362445,\n",
      "        -0.71428571,  0.18412663],\n",
      "       [-0.0826437 ,  0.00542337, -0.04778182, ..., -1.13362445,\n",
      "        -0.71428571,  0.18412663]]), 666       0.0\n",
      "667       6.0\n",
      "668       5.0\n",
      "669       2.0\n",
      "670       1.0\n",
      "         ... \n",
      "260278    0.0\n",
      "260279    2.0\n",
      "260280    0.0\n",
      "260281    2.0\n",
      "260282    1.0\n",
      "Name: new_deaths, Length: 23850, dtype: float64)\n",
      "scaled test array for Split 5:\n",
      "[[-0.12500013 -0.05475857 -0.08227172 ... -0.8419214  -0.94594595\n",
      "   0.97561958]\n",
      " [-0.12497013 -0.04671099 -0.08182127 ... -0.8419214  -0.94594595\n",
      "   0.97561958]\n",
      " [-0.12494492 -0.04951015 -0.08112479 ... -0.8419214  -0.94594595\n",
      "   0.97561958]\n",
      " ...\n",
      " [-0.08006566 -0.02396781 -0.06990122 ... -1.13362445 -0.71428571\n",
      "   0.18412663]\n",
      " [-0.07991504  0.02361791 -0.06994223 ... -1.13362445 -0.71428571\n",
      "   0.18412663]\n",
      " [-0.07991504 -0.06420574 -0.06994223 ... -1.13362445 -0.71428571\n",
      "   0.18412663]]\n",
      "----------------------------------------\n",
      "scaled train array for Split 6:\n",
      "(array([[-1.60043672e-01,  2.06067544e-01,  2.62026760e-02, ...,\n",
      "        -8.41921397e-01, -9.45945946e-01,  9.75619579e-01],\n",
      "       [-1.60019797e-01,  1.30509445e-01,  5.85760957e-04, ...,\n",
      "        -8.41921397e-01, -9.45945946e-01,  9.75619579e-01],\n",
      "       [-1.59986209e-01,  1.85460790e-01,  3.83634740e-03, ...,\n",
      "        -8.41921397e-01, -9.45945946e-01,  9.75619579e-01],\n",
      "       ...,\n",
      "       [-1.29196595e-01, -4.57927876e-03, -7.80882619e-02, ...,\n",
      "        -1.13362445e+00, -7.14285714e-01,  1.84126625e-01],\n",
      "       [-1.29196595e-01, -4.57927876e-03, -7.80882619e-02, ...,\n",
      "        -1.13362445e+00, -7.14285714e-01,  1.84126625e-01],\n",
      "       [-1.29196595e-01, -4.57927876e-03, -7.80882619e-02, ...,\n",
      "        -1.13362445e+00, -7.14285714e-01,  1.84126625e-01]]), 846       0.0\n",
      "847       1.0\n",
      "848       1.0\n",
      "849       0.0\n",
      "850       0.0\n",
      "         ... \n",
      "260458    0.0\n",
      "260459    0.0\n",
      "260460    0.0\n",
      "260461    0.0\n",
      "260462    0.0\n",
      "Name: new_deaths, Length: 23850, dtype: float64)\n",
      "scaled test array for Split 6:\n",
      "[[-0.15067385  0.2152261   0.01631984 ... -0.8419214  -0.94594595\n",
      "   0.97561958]\n",
      " [-0.1506083   0.3663423   0.01228915 ... -0.8419214  -0.94594595\n",
      "   0.97561958]\n",
      " [-0.15059737  0.05724098  0.00214597 ... -0.8419214  -0.94594595\n",
      "   0.97561958]\n",
      " ...\n",
      " [-0.12860456 -0.00457928 -0.07808826 ... -1.13362445 -0.71428571\n",
      "   0.18412663]\n",
      " [-0.12860456 -0.00457928 -0.07808826 ... -1.13362445 -0.71428571\n",
      "   0.18412663]\n",
      " [-0.12860456 -0.00457928 -0.07808826 ... -1.13362445 -0.71428571\n",
      "   0.18412663]]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "death_columns = [col for col in data_clean.columns if 'deaths' in col]\n",
    "\n",
    "scaled_train_sets = []\n",
    "scaled_test_sets = []\n",
    "\n",
    "for i, (train_dates, test_dates) in enumerate(splits):\n",
    "    # Filter the original DataFrame based on the train and test dates\n",
    "    train_data = data_clean[data_clean['date'].isin(train_dates)]\n",
    "    test_data = data_clean[data_clean['date'].isin(test_dates)]\n",
    "\n",
    "    # Extract features and target for train data\n",
    "    X_train = train_data.drop(columns = ['iso_code', 'continent', 'location', 'date'] + death_columns)\n",
    "    y_train = train_data['new_deaths']\n",
    "    \n",
    "    # Extract features for test data, but without label columns\n",
    "    X_test = test_data.drop(columns = ['iso_code', 'continent', 'location', 'date'] + death_columns)\n",
    "    \n",
    "    # Initialize and fit the scaler on the training data\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Append the scaled arrays to respective lists\n",
    "    scaled_train_sets.append((X_train_scaled, y_train))\n",
    "    scaled_test_sets.append(X_test_scaled)\n",
    "    \n",
    "    print(f\"scaled train array for Split {i+1}:\")\n",
    "    print(scaled_train_sets[i])\n",
    "    print(f\"scaled test array for Split {i+1}:\")\n",
    "    print(scaled_test_sets[i])\n",
    "    print(\"----------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 6 scaled splits saved in the lists scaled_train_sets and scaled_test_sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#normalizing the cleaned data with MinMaxScaler (assuming there are no/few outliers)\n",
    "#scaler_minmax = MinMaxScaler()\n",
    "#data_clean_normalized_minmax = data_clean.copy()\n",
    "#data_clean_normalized_minmax[numerical_features] = scaler_minmax.fit_transform(data_clean[numerical_features])\n",
    "#print(data_clean_normalized_minmax)\n",
    "data_clean_numerical = data_clean[numerical_features]\n",
    "#normalizing the cleaned data with RobustScaler (assuming there are outliers)\n",
    "scaler_robust = RobustScaler()\n",
    "data_clean_normalized_robust = data_clean.copy()\n",
    "data_clean_normalized_robust[numerical_features] = scaler_robust.fit_transform(data_clean_numerical)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_standard = StandardScaler()\n",
    "data_clean_standardized = data_clean.copy()\n",
    "data_clean_standardized[numerical_features] = scaler_standard.fit_transform(data_clean[numerical_features])\n",
    "#print(data_clean_standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Total Deaths and Total Cases are both numerical values, therefore mainly regression is suitable, regression models that could be implemented are:\n",
    "\n",
    "1) **traditional linear regression (lasso/ridge)**\n",
    "\n",
    "- *Simplicity and Interpretability*: These models are straightforward to implement and interpret, making it easy to understand the relationship between predictors and the target variables.\n",
    "\n",
    "- *Handling Multicollinearity*: Lasso and Ridge can manage multicollinearity among features, improving the model's stability and performance.\n",
    "    \n",
    "- *Feature Selection*: Lasso performs automatic feature selection by shrinking some coefficients to zero, helping identify the most significant predictors.\n",
    "\n",
    "2) **random forest regression**\n",
    "\n",
    "- *Non-Linear Relationships:* This model captures complex non-linear interactions between features, which is crucial for accurately modeling real-world phenomena like the spread of a virus.\n",
    "\n",
    "- *Robustness and Generalization:* By averaging the predictions of multiple decision trees, Random Forests reduce overfitting and provide robust predictions.\n",
    "\n",
    "- *Feature Importance:* It offers insights into feature importance, helping to understand which factors most influence total cases and deaths.\n",
    "\n",
    "3) **support vector regression**\n",
    "\n",
    "- *Handling Non-Linearities:* SVR can model complex relationships using various kernels, making it suitable for datasets with non-linear patterns.\n",
    "\n",
    "- *Margin Maximization:* It focuses on minimizing prediction error within a margin, providing a balance between accuracy and generalization.\n",
    "\n",
    "- *Effective in High-Dimensional Spaces:* SVR performs well with high-dimensional data, which can be beneficial when dealing with multiple predictors for total deaths and total cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Traditional Regression Analysis\n",
    "\n",
    "- Use multiple linear regression or other suitable regression techniques.\n",
    "\n",
    "- Examine the relationship between total_cases/total_deaths and various predictors like:\n",
    "\n",
    "    -stringency index\n",
    "\n",
    "    -vaccination rates\n",
    "\n",
    "    -demographic factors\n",
    "\n",
    "    -regionality\n",
    "\n",
    "2) Short-Term Predictions\n",
    "\n",
    "- Use time-series forecasting techniques such as ARIMA, Prophet, or other regression-based forecasting models.\n",
    "\n",
    "- Make short-term predictions for total_cases and total_deaths by cutting out recent time periods.\n",
    "\n",
    "3) Classification possible?\n",
    "\n",
    "- Only 'continent' as viable option\n",
    "\n",
    "- logistic regression, random forest, SVM, KNN\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
